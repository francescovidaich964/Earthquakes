{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import linalg as la\n",
    "from scipy import optimize\n",
    "from scipy import stats\n",
    "from scipy.stats import chisquare\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Earthquakes as E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = E.ImportDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E.ImportLibraries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Understanding data\n",
    "\n",
    "First of all we took a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ['event', 'prev_event', 'time', 'magnitude', 'x', 'y', 'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the range of magnitudes is [2,7.3] \n",
    "and that the measurement of time starts from 0 with the first event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['magnitude']>6].sort_values(by = 'magnitude', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not all the greatest earthquakes are without ancestors (we shall call them 'prime events')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Time distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just a routine look at the time distribution and time scale of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E.EventVsTime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Magnitude distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_expon(x, No, alpha):\n",
    "    return No*np.exp(-alpha*x)\n",
    "\n",
    "def my_log_expon(x, Q, alpha):\n",
    "    return -alpha*x + Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can see how the magnitude is distributed as exp(-m)\n",
    "n, bins, _ = plt.hist(df['magnitude'], log = True, histtype = 'step')\n",
    "bin_centers = (bins[1:] + bins[:-1])/2\n",
    "plt.errorbar(bin_centers, n, np.sqrt(n), fmt = '.b', label = 'values with \\nPoisson error')\n",
    "\n",
    "params1, _ = optimize.curve_fit(my_log_expon, bin_centers, np.log(n))\n",
    "[Q, alpha] = params1\n",
    "No = int(np.exp(Q))\n",
    "plt.plot(bin_centers, my_expon(bin_centers, No, alpha), \n",
    "         label = 'f($M_w$) = $N_o$$e^{-aM_w}$ '+'\\n$N_o$ = {}\\n a = {}'.format(No,round(alpha,2)))\n",
    "\n",
    "plt.xlabel('magnitude [$M_w$]')\n",
    "plt.ylabel('occurrencies [log scale]')\n",
    "plt.ylim(bottom = 1)\n",
    "plt.legend(fontsize = 12)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ADD NON LOGSCALE PLOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Spatial distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#az = [i*15 for i in range(24)]\n",
    "#for azim in az:\n",
    "#%matplotlib nbagg\n",
    "m_bar = 3\n",
    "dfm = df[df['magnitude'] > m_bar]\n",
    "x_coord = dfm['x']\n",
    "y_coord = dfm['y']\n",
    "z_coord = dfm['z']\n",
    "magnitude = dfm['magnitude']\n",
    "\n",
    "fig = plt.figure(figsize = (7,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x_coord/1e06, y_coord/1e06, z_coord/1e06, \n",
    "           s=np.exp(magnitude*1.5)/np.exp(3), c = magnitude, marker=\"o\", alpha = 0.5, \n",
    "           label = 'size of marker \\nproportional \\nto magnitude', cmap = 'plasma')\n",
    "ax.set_xlabel(\"x [$10^6$ m]\", fontsize = 16)\n",
    "ax.set_ylabel(\"y [$10^6$ m]\", fontsize = 16)\n",
    "ax.set_zlabel(\"z [$10^6$ m]\", fontsize = 16)\n",
    "ax.set_title('Spatial distribution of events for m > 3', fontsize = 20)\n",
    "ax.legend(loc = 'center left')\n",
    "#print('azim = ', azim)\n",
    "ax.view_init(elev = 30, azim = 330)\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 PCA for spatial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['x','y','z']].values.T\n",
    "X = X.astype(\"float64\")\n",
    "\n",
    "# centering and rescaling the coordinates\n",
    "for i in range(3):\n",
    "    X[i] = (X[i] - X[i].mean())/X[i].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA \n",
    "C = np.cov(X)\n",
    "U, spectrum, Vt = la.svd(C)\n",
    "print ('Spectrum: ', spectrum,'\\n')\n",
    "perc_expl2 = (spectrum[0] + spectrum[1]) / spectrum.sum()\n",
    "print('Percent of the total variability explained considering the two main features: ', perc_expl2, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the two largest eigenvalues can account for 99.9% of the total variability of the spatial coordinates; thus we can effectively conclude that the events are located on a plane.\n",
    "\n",
    "## DA CAPIRE:\n",
    "il piano è parallelo o \"perpendicolare\" (o trasversale) alla superficie terrestre? Basterebbe anche solo leggere qualcuno che l'ha già spiegato, dato che non è banale capirlo in queste coordinate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following panels we represent the two-dimensional projections of the spatial coordinates of the dataset both for the x,y,z frame and the $v_0, v_1, v_2$ frame (e.g. the one in the PCA coordinates); in the latter one we can see how the $v_2$ component is almost constant and all the relevant information is contained in the scatterplot $v_0$ vs $v_1$, that represents the projection of the original data along the eigenvectors associated to the two largest eigenvalues found in the PCA procedure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projecting the data in the plane generated by the two eigenvectors with the largest eigenvalues\n",
    "Xp = np.dot(Vt,X)\n",
    "\n",
    "%matplotlib inline\n",
    "# general plot of the PCA\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3,\n",
    "                               figsize=(12, 6))\n",
    "\n",
    "[[ax01, ax02, ax03],[ax11, ax12, ax13]] = axes\n",
    "\n",
    "ax01.scatter(X[0], X[1], s= 5)\n",
    "ax01.set_title('x-y projection')\n",
    "ax01.set_xlabel('x')\n",
    "ax01.set_ylabel('y')\n",
    "\n",
    "ax02.scatter(X[0], X[2], s= 5)\n",
    "ax02.set_title('x-z projection')\n",
    "ax02.set_xlabel('x')\n",
    "ax02.set_ylabel('z')\n",
    "\n",
    "ax03.scatter(X[1], X[2], s= 5)\n",
    "ax03.set_title('y-z projection')\n",
    "ax03.set_xlabel('y')\n",
    "ax03.set_ylabel('z')\n",
    "\n",
    "ax11.scatter(Xp[0], Xp[1], s= 5)\n",
    "ax11.set_title('$v_0$-$v_1$ projection')\n",
    "(y_bottom, y_top) = ax11.get_ylim()\n",
    "ax11.set_xlabel('$v_0$')\n",
    "ax11.set_ylabel('$v_1$')\n",
    "\n",
    "ax12.scatter(Xp[0], Xp[2], s= 5)\n",
    "ax12.set_ylim(y_bottom, y_top)\n",
    "ax12.set_title('$v_0$-$v_2$ projection')\n",
    "ax12.set_xlabel('$v_0$')\n",
    "ax12.set_ylabel('$v_2$')\n",
    "\n",
    "ax13.scatter(Xp[1], Xp[2], s= 5)\n",
    "ax13.set_ylim(y_bottom, y_top)\n",
    "ax13.set_title('$v_1$-$v_2$ projection')\n",
    "ax13.set_xlabel('$v_1$')\n",
    "ax13.set_ylabel('$v_2$')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ABBELLIRE -> fontsize maggiore, magari inserire lieve trasparenza per visualizzare anche la densità di eventi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMMENTO KDE: \n",
    "onestamente non so cosa ci dica questo grafico, io mi aspettavo di vedere le faglie sulla KDE di destra, che invece sembra molto più uniforme della KDE in alto. Secondo me la direzione di maggiore varianza (autovalore maggiore) è Xp[0] e mi sembra quella parallela alle faglie, non quella trasversale; però la KDE in alto sembra proprio quella che intuitivamente mi sarei aspettato \"schiacciando\" le faglie in un istogramma... Sarebbe da verificare plottando nello spazio originale il vettore associato all'autovalore maggiore (volendo anche al secondo) e vedere se è parallelo o trasversale alle faglie. Se non sappiamo spiegarci la KDE e non ci aiuta a spiegare nulla, la toglierei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Show the joint distribution using kernel density estimation\n",
    "g = sns.jointplot(Xp[0], Xp[1], kind=\"kde\", xlim=[-4,4], ylim=[-4,4], height=7, space=0)\n",
    "g.set_axis_labels(xlabel='x', ylabel='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the x,y,z frame, we can see in a trasversal view how the plane that corresponds to the eigenspace of the two largest eigenvectors fits the data. This is further evidence of the PCA efficacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[v1, v2, v3] = Vt\n",
    "normal = v3\n",
    "xx, yy = np.meshgrid(range(-4,5), range(-4,5))# calculate corresponding z\n",
    "z = (-normal[0] * xx - normal[1] * yy ) * 1. /normal[2]\n",
    "\n",
    "m_bar = 3\n",
    "dfm = df[df['magnitude'] > m_bar]\n",
    "x_coord = dfm['x']\n",
    "y_coord = dfm['y']\n",
    "z_coord = dfm['z']\n",
    "x_coord = (x_coord - x_coord.mean())/x_coord.std()\n",
    "y_coord = (y_coord - y_coord.mean())/y_coord.std()\n",
    "z_coord = (z_coord - z_coord.mean())/z_coord.std()\n",
    "magnitude = dfm['magnitude']\n",
    "\n",
    "#%matplotlib nbagg\n",
    "\n",
    "fig = plt.figure(figsize = (7,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(xx, yy, z, alpha=0.3)\n",
    "ax.scatter(x_coord, y_coord, z_coord, \n",
    "           s=np.exp(magnitude*1.5)/np.exp(3), c = magnitude, marker=\"o\", alpha = 0.5, \n",
    "           label = 'size of marker \\nproportional \\nto magnitude', cmap = 'plasma')\n",
    "ax.set_xlabel(\"x [$10^6$ m]\", fontsize = 16)\n",
    "ax.set_ylabel(\"y [$10^6$ m]\", fontsize = 16)\n",
    "ax.set_zlabel(\"z [$10^6$ m]\", fontsize = 16)\n",
    "ax.set_title('Spatial distribution of events for m > 3', fontsize = 20)\n",
    "ax.legend(loc = 'center right')\n",
    "#print('azim = ', azim)\n",
    "ax.view_init(elev = 15, azim = 0)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plotted a heatmap of the events using as coordinates the first and the second principal components, to visualize the density of events in those coordinates. The color is brighter where the density is higher, with a scale of colors that is logaritmic in the occurrencies of each cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "plt.figure(211)\n",
    "\n",
    "plt.hist2d(Xp[0], Xp[1], bins = 50, norm = LogNorm(), cmap = \"plasma\")\n",
    "plt.colorbar()\n",
    "plt.xlabel('First principal component', fontsize = 14)\n",
    "plt.ylabel('Second principal component', fontsize = 14)\n",
    "plt.title('Heatmap of events in P.C. coordinates', fontsize = 16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Tree architecture\n",
    "\n",
    "Here we want to exploit the information contained in the 'prev_event' column in order to be able to compute quantities such as distance and waiting time between an event and his ancestor.\n",
    "\n",
    "First of all we iteratively created a nested dictionary, containing as keys the IDs of the primes events (i.e. the ones labelled with 'prev_event' = 1) and as values a dictionary with:\n",
    "* the \"depth\" of the event in the cause-effect chain (i.e. 0 for the prime events, 1 for the ones caused by them, 2 for the ones caused by events of depth 1 and so on so forth);\n",
    "* the \"children\" of that event, i.e. the events that have as prev_event the id of the event considered; \"children\" value contains itself a dictionary like the one just described for the prime events, hence the nested structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evees_light(df, index=-1, depth=0):\n",
    "    dataset = {}\n",
    "    df_filtered = df[df[\"prev_event\"]==index]\n",
    "    lenght =  df_filtered.shape[0]\n",
    "    for i in range(lenght):\n",
    "        event = df_filtered.iloc[i]\n",
    "        dataset[str(int(event[\"event\"]))] = {\"depth\" : depth}\n",
    "        dataset[str(int(event[\"event\"]))][\"children\"] = evees_light(df, index=int(event[\"event\"]), depth = depth + 1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we mapped the dictionary so obtained in a list of dictionaries, whose indexes are the depth of the events (ranging from 0 to max_depth, that is unknown a priori); the entry of the list of index i is a dictionary that has as keys the IDs of the events of depth = i and as values the IDs of its children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dict_array(dataset, dict_array):\n",
    "    for k in dataset.keys():\n",
    "        depth = dataset[k]['depth']\n",
    "        # k is the keyword, children_ids are the values\n",
    "        if len(dict_array) < depth + 1 :\n",
    "            while len(dict_array) < depth + 1:\n",
    "                dict_array.append({})\n",
    "        else:\n",
    "            dict_array[depth][k] = list(dataset[k]['children'].keys())\n",
    "        if len(list(dataset[k]['children'].keys())) != 0:\n",
    "            write_dict_array(dataset[k]['children'], dict_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = evees_light(df)\n",
    "v_dict = [{}]\n",
    "write_dict_array(tree_dataset, v_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the list of dictionaries to add the depth and the number of children (or number of edges if we think of the dataset as an oriented graph) of each event by adding two columns to the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = df.shape[0]\n",
    "edges = np.zeros(N)\n",
    "depths = np.zeros(N)\n",
    "\n",
    "for d in range(len(v_dict)):\n",
    "    for k in v_dict[d].keys():\n",
    "        #print(k)\n",
    "        edges[int(k)] = len(v_dict[d][k])\n",
    "        depths[int(k)] = d\n",
    "        \n",
    "df['edges'] = edges\n",
    "df['depth'] = depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the new features we can see an interesting thing: the event with the highest magnitude (7.3) is not prime \n",
    "# (because it hasn't prev_event = -1), but it happend after a concatenation of 11 earthquakes and subsequently caused \n",
    "# other 4209 events!\n",
    "df[df['magnitude'] == 7.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Furthermore we can see that depth isn't a good indicator of magnitude and that in fact one can't find any relation\n",
    "# between the two\n",
    "mean_mag = df.groupby('depth')['magnitude'].mean()\n",
    "depth_axis = [i for i in range(len(mean_mag))]\n",
    "plt.plot(depth_axis, mean_mag)\n",
    "plt.xlabel('cause-effect depth', fontsize = 14)\n",
    "plt.ylabel('mean magnitude', fontsize = 14)\n",
    "plt.show()\n",
    "\n",
    "# ABBELLIRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we could expect the events with higher magnitudes have much more edges than the ones with lower magnitudes. \n",
    "# In fact the dependence is exponential with m\n",
    "\n",
    "def my_expon1(x, No, gamma):\n",
    "    return No*np.exp(gamma*x)\n",
    "\n",
    "plt.plot(df['magnitude'], df['edges'] , '.b')\n",
    "plt.xlabel('magnitude [$M_w$]', fontsize = 14)\n",
    "plt.ylabel('number of edges', fontsize = 14)\n",
    "\n",
    "df5 = df[df['magnitude'] > 5]\n",
    "ax = plt.axes([0.3, 0.4, 0.4, 0.4])\n",
    "ax.set_title('Zoom for m > 5')\n",
    "ax.plot(df5['magnitude'], df5['edges'] , '.g')\n",
    "params1, _ = optimize.curve_fit(my_expon1, df5['magnitude'], df5['edges'])\n",
    "[No, gamma] = params1\n",
    "x_axis = np.linspace(5,7.3,100)\n",
    "ax.plot(x_axis, my_expon1(x_axis, *params1), 'r-', \n",
    "        label = '$N_0$exp($\\gamma m$):\\n $\\gamma$ = {}'.format(round(gamma,2)))\n",
    "ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again depth isn't a good indicator of edges (as it wasn't for magnitude)\n",
    "plt.plot(df['depth'], df['edges'], 'b.')\n",
    "plt.xlabel('cause-effect depth', fontsize = 14)\n",
    "plt.ylabel('number of edges', fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Distribution of waiting time \n",
    "Compute the distribution $P_m(t)$ of waiting times for events of magnitude m or above (i.e. do not consider events below $m$). In shaping the bin sizes, take into account that this distribution is expected to have a power-law decay with time (e.g $\\sim 1/t$), and that a power-law is well visualized in log-log scale. Do this analysis for many values of $m$, say $m=2,3,4,5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning(x, rescaling = False, density = False):\n",
    "    \n",
    "    \"\"\"Binning for power laws distributions.\n",
    "        x = entries generated from a power law.\"\"\"\n",
    "    \n",
    "    # x must have streactly positive values; x isn't normalized in general\n",
    "    x = x[x>0]\n",
    "    if rescaling == True:\n",
    "        x = x/x.max()\n",
    "        \n",
    "    x.sort()\n",
    "    \n",
    "    # empirical method to get a good amount of bins (min 8, max 16), depending on the number of samples\n",
    "    bin_extremes_number = max( min( int( np.log(len(x))*2 ) , 17), 9) \n",
    "    \n",
    "    # choose the right extreme of the first bin as the one that corresponds to N/bin_extreme_number of samples in the\n",
    "    #first bin\n",
    "    first_quantile = x[int(x.shape[0]*(1/bin_extremes_number))]\n",
    "    \n",
    "    # create bins whose widths are constant in the logaritmic scale \n",
    "    bin_extremes = np.logspace(np.log10(first_quantile), np.log10(x.max()), bin_extremes_number)\n",
    "    #and then concatenate 0 as a starting point for the first bin\n",
    "    bin_extremes = np.concatenate((np.array([0]), bin_extremes))\n",
    "\n",
    "    widths = bin_extremes[1:] - bin_extremes[:-1]\n",
    "    centers = (bin_extremes[1:] + bin_extremes[:-1])/2\n",
    "    \n",
    "    freq, _, _ = plt.hist(x, bins=bin_extremes)\n",
    "    plt.close()\n",
    "    \n",
    "    # having unhomogeneous bins the frequencies must be divided for the width of the corresponding bin in order\n",
    "    # to get an estimator of the probability density in each point (~ bin center), i.e. :\n",
    "    # weights(bin_center) = Prob(bin_center) , if normalized = True\n",
    "    weights = freq/widths\n",
    "    # poissonian errors for the frequencies, then the error is propagated to the weights dividing elementwise for\n",
    "    # the widths\n",
    "    sigma_weights = np.sqrt(freq)/widths\n",
    "    \n",
    "    # merging of the first two bins until we get that the first bin represents the max of the PDF \n",
    "    # this is a useful option to regularize the first bin if we expect it to assume the highest value\n",
    "    while weights[0] < weights[1]:\n",
    "        print('Merging first and second bins.')\n",
    "        #this is done by removing the second extreme, thus the first bin becomes the one between 0 and 2\n",
    "        bin_extremes = np.concatenate(([bin_extremes[0]], bin_extremes[2:] ))\n",
    "        widths = bin_extremes[1:] - bin_extremes[:-1]\n",
    "        centers = (bin_extremes[1:] + bin_extremes[:-1])/2\n",
    "        \n",
    "        # then of course we need to recompute frequencies and weights for the new bins\n",
    "        freq, _, _ = plt.hist(x, bins=bin_extremes)\n",
    "        plt.close()\n",
    "        weights = freq/widths\n",
    "        sigma_weights = np.sqrt(freq)/widths\n",
    "        \n",
    "    # adding also the merging of empty bins with the one on the left\n",
    "    # this avoids the issue of taking the log of 0 in the log-log space where we perform the linear regression\n",
    "    mask = (freq != np.zeros(len(freq)))\n",
    "    flag = np.all(mask)\n",
    "    # should enter in the while loop only in there is at least one bin without counts in it\n",
    "    \n",
    "    while flag == False:\n",
    "        print('Entered in the while loop.')\n",
    "        print('Original frequencies: ', freq)\n",
    "        for i in range(1,len(freq)):\n",
    "            if freq[i] == 0:\n",
    "                print('Merging bin {} (empty) with bin {}.'.format(i,i-1))\n",
    "                # bin extremes should be of length len(freq) + 1\n",
    "                # notice that bin_extremes[i] corresponds to the right border of bin[i-1]\n",
    "                # bin_extremes[:i] excludes the bin_extreme[i] !\n",
    "                bin_extremes = np.concatenate((bin_extremes[:i], bin_extremes[i+1:] ))\n",
    "                widths = bin_extremes[1:] - bin_extremes[:-1]\n",
    "                centers = (bin_extremes[1:] + bin_extremes[:-1])/2\n",
    "                # call a break of the for because the len frequence changes and can result in index errors\n",
    "                break\n",
    "                \n",
    "        # update of the frequencies\n",
    "\n",
    "        freq, _, _ = plt.hist(x, bins=bin_extremes)\n",
    "        plt.close()\n",
    "        weights = freq/widths\n",
    "        sigma_weights = np.sqrt(freq)/widths\n",
    "        \n",
    "        # update of the exit condition\n",
    "        mask = (freq != np.zeros(len(freq)))\n",
    "        flag = np.all(mask)\n",
    " \n",
    "    if density == True:\n",
    "        #returns normalized weights (with rescaled errors) so that the area of the histogram is 1\n",
    "        area = np.sum(weights*widths)\n",
    "        weights = weights / area\n",
    "        sigma_weights = sigma_weights/area\n",
    "    \n",
    "    return bin_extremes, widths, centers, freq, weights, sigma_weights   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_f(x, p, q):\n",
    "        return p*x+q\n",
    "\n",
    "def loglog_fitting(x, y, skip_initial_pt = 1, cut_off = False, P0 = 3 ):\n",
    "    from scipy import optimize\n",
    "    \n",
    "    # this is used because the first bin is always problematic: in the log space log(0) doesn't exist, but in the \n",
    "    # original space it is the only non-arbitrary lower bound to the values that are acceptable both for the waiting times\n",
    "    # and the distances. Thus by default we ignore the first point in the regression\n",
    "    x = x[skip_initial_pt:]\n",
    "    y = y[skip_initial_pt:]\n",
    "    \n",
    "    if cut_off == False:\n",
    "        params, cov = optimize.curve_fit(linear_f, x, y)\n",
    "        [p,q] = params\n",
    "        return p, q, cov\n",
    "    \n",
    "    else:\n",
    "        #notice that we are already working without considering the first skip_initial_pt points\n",
    "        mean_squared_res = []    \n",
    "        predicted_squared_res = []\n",
    "        \n",
    "        # P is the number of points considered for the fit, used for testing whether the point P+1 is alligned\n",
    "        # with them or not; the likelihood of not being alligned is given by the ratio between \n",
    "        # the predicted squared residual of point P+1 (based on the fit on the previous P points) \n",
    "        # and the mean squared residual computed for the P points \n",
    "        # P0 is the minumum amount of points that we require to be aligned \"a priori\" and it is used to calibrate\n",
    "        # the notion of alligned/not-alligned for the fit (more points, like 5, make the algorithm more stable w.r.t.\n",
    "        # statistical fluctuations)\n",
    "        for P in range(P0,len(x)):\n",
    "            params, cov = optimize.curve_fit(linear_f, x[:P], y[:P])\n",
    "            [p,q] = params\n",
    "\n",
    "            squared_residuals = np.power(y[:P] - linear_f(x[:P], *params),2)\n",
    "            mean_squared_res.append(squared_residuals.mean())\n",
    "            \n",
    "            next_pt_squared_res = np.power(y[P] - linear_f(x[P], *params),2) \n",
    "            predicted_squared_res.append(next_pt_squared_res)\n",
    "            \n",
    "        predicted_squared_res = np.array(predicted_squared_res)\n",
    "        mean_squared_res = np.array(mean_squared_res)\n",
    "        predicted_vs_mean_ratio = predicted_squared_res/mean_squared_res\n",
    "        predicted_vs_mean_ratio_norm = predicted_vs_mean_ratio/predicted_vs_mean_ratio.max()\n",
    "    \n",
    "        # The index of the array predicted_vs_mean_ratio_norm is shifted by P0 positions, meaning that we consider\n",
    "        # as the maximal number of alligned points the one corresponding to the index of the max of \n",
    "        # predicted_vs_mean_ratio_norm + P0 = good_points. \n",
    "\n",
    "        indexes = np.arange(P0,len(x))\n",
    "        max_index = indexes[predicted_vs_mean_ratio_norm == predicted_vs_mean_ratio_norm.max()]\n",
    "        good_points = max_index[0]\n",
    "        print('Good points {} out of {}'.format(good_points, len(x)))\n",
    "        # Then the cut-off is estimated as x_cut = (x[good_points]+ x[good_points+1])/2\n",
    "        if good_points < len(x):\n",
    "            # the -1 shift at the index is because the index numeration starts at 0\n",
    "            x_cut = (x[good_points-1]+ x[good_points])/2\n",
    "        else:\n",
    "            print('ATTENTION: all points seem alligned.')\n",
    "            print('x_cut set to the value of the last point.')\n",
    "            x_cut = x[-1]\n",
    "            \n",
    "        params, cov = optimize.curve_fit(linear_f, x[:good_points], y[:good_points])\n",
    "        [p,q] = params\n",
    "        \n",
    "        return p, q, cov, x_cut\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_powerlaw_hist(x, rescaling = False, density = False, show = True, **kwargs):\n",
    "    \n",
    "    # compute automatically a suitable binning for x and all the associated quantities\n",
    "    bin_extremes, widths, centers, freq, weights, sigma_weights = binning(x, rescaling, density)\n",
    "    bin_number = len(centers)\n",
    "    \n",
    "    if show:\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,figsize=(11, 5))\n",
    "        \n",
    "        # we plot a single point for each bin with the weight being the probability density estimated for the center \n",
    "        # of the bin (see binning function)\n",
    "        ax1.hist(centers, bins = bin_extremes, weights = weights, histtype = 'step')\n",
    "        ax1.errorbar(centers, weights, sigma_weights, fmt = 'r.')\n",
    "        ax1.set_xscale('log')\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.set_xlabel('waiting times [s]', fontsize = 14)\n",
    "        ax1.set_ylabel('occurrencies [a.u]', fontsize = 14)\n",
    "        ax1.set_title('Number of events = {}'.format(len(x)))\n",
    "    \n",
    "    # now we need to fit the power law in the log-log space, eventually identifying the points before the cut-off\n",
    "    # this should work automatically both for the case rescaling = True or False (if True, x is in [0,1]) \n",
    "    # and for the case density = True or False (if True, the area of the histogram is normalized to 1 \n",
    "    # and the weights are rescaled so that np.sum(weights*bin_widths) = 1)\n",
    "    log_x = np.log(centers)\n",
    "    log_w = np.log(weights)\n",
    "    \n",
    "    # the idea is to write a function that as a default just fits the (log_x,log_w) with a linear function \n",
    "    # log_w = p*log_x + q and has 2 flags: one for excluding skip_initial_pt points (set to 1 for default because\n",
    "    # the first bin is always problematic) and another one to signal that we expect a cut-off at the right side of the\n",
    "    # distribution (i.e. the tail) and we want to stop fitting just before the cut-off.\n",
    "    # we want as a return the parameters p and q with their covariance matrix (that is the default return of \n",
    "    # scipy curve_fit) and, if the cut_off flag is True, also the estimate cut-off (rescaled or not depending on the \n",
    "    # setting passed before)\n",
    "    \n",
    "    if 'cut_off' in kwargs:\n",
    "        if kwargs['cut_off'] == True:\n",
    "             p, q, cov, log_x_cut = loglog_fitting(log_x, log_w, **kwargs)\n",
    "    else:\n",
    "        p, q, cov = loglog_fitting(log_x, log_w, **kwargs)\n",
    "    \n",
    "    if show:\n",
    "        y_errors = sigma_weights/weights\n",
    "\n",
    "        ax2.errorbar(log_x, log_w, yerr = y_errors ,fmt ='r.', label = 'entries with errors')\n",
    "        ax2.plot(log_x, linear_f(log_x, p, q), \n",
    "                 label = 'f(x) = px + q\\np = {} \\nq = {}'.format(round(p,1),round(q,1)))\n",
    "        ax2.legend()\n",
    "        ax2.set_xlabel('waiting times [logscale]', fontsize = 14)\n",
    "        ax2.set_ylabel('occurrencies [logscale]', fontsize = 14)\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    if 'cut_off' in kwargs:\n",
    "        if kwargs['cut_off'] == True:\n",
    "            if rescaling == True:\n",
    "                return p, q, np.sqrt(cov[0,0]), np.exp(log_x_cut)*x.max()\n",
    "            else:\n",
    "                return p, q, np.sqrt(cov[0,0]), np.exp(log_x_cut)\n",
    "    else:\n",
    "        # returns the slope, the intercept and the error of the slope\n",
    "        return p, q, np.sqrt(cov[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we choose to study the data for m between 2 and 4.5, because above 5 we get just 104 samples \n",
    "# and there isn't enough information to fit anything but the initial peak of the distribution.\n",
    "ms1 = np.linspace(2,4.5,11)\n",
    "p_time = np.zeros(len(ms1))\n",
    "q_time = np.zeros(len(ms1))\n",
    "p_t_errors = np.zeros(len(ms1))\n",
    "cut_times = np.zeros(len(ms1))\n",
    "\n",
    "for i in range(len(ms1)):\n",
    "\n",
    "    m = ms1[i]\n",
    "    print('\\nTime distribution for m = ', m, '\\n')\n",
    "    # waiting time for events of magnitude > m\n",
    "    dfm = df[df['magnitude'] > m]\n",
    "    timem = np.array(dfm['time'])\n",
    "    timem.sort()\n",
    "    time_d = timem[1:] - timem[:-1]\n",
    "    \n",
    "    # eliminating a couple of anomalous events\n",
    "    temp = time_d[time_d != time_d.max()]\n",
    "    maximum = temp.max()\n",
    "    if time_d.max()*3/4 > maximum:\n",
    "        time_d = temp\n",
    "\n",
    "    p_time[i], q_time[i], p_t_errors[i], cut_times[i] = plot_powerlaw_hist(time_d, rescaling = False, \n",
    "                                                                   density = True, cut_off = True, P0 = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the analysis of the p exponent of $P(t) \\propto t^p$ we can't find any significant pattern in the dipendence from the magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(ms1, p_time)\n",
    "plt.errorbar(ms1, p_time, yerr = p_t_errors, fmt = '.r', label = 'estimated exponents \\nwith errors' )\n",
    "plt.plot(ms1, intercept+slope*ms1, label = 'fit: p(m) = %.2fm%.2f'%(slope,intercept))\n",
    "plt.ylabel('exponent p', fontsize = 14)\n",
    "plt.xlabel('magnitude m', fontsize = 14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead the cut-off time has an exponential dependence from the magnitude. We will see in the last section how this relation can be used for a rescaling procedure that collapses the distributions of the waiting times for each magnitude to a single, magnitude-independent, distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(ms1, np.log(cut_times))\n",
    "predicted_cut_times = np.exp(intercept+slope*ms1)\n",
    "plt.plot(ms1, predicted_cut_times, label = 'fit: p(m) = %.2fm + %.2f'%(slope,intercept))\n",
    "\n",
    "plt.errorbar(ms1, cut_times, fmt = '.r')\n",
    "plt.ylabel('Waiting time cut-off [s]', fontsize = 14)\n",
    "plt.xlabel('Magnitude m [$T_w$]', fontsize = 14)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTA: \n",
    "io toglierei tutta la parte qui sotto, sia per i concatenated events che per i prime events, perché tanto il caso con tutti gli eventi funziona già bene (meglio) di suo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Waiting time distribution between concatenated events - REMOVE\n",
    "\n",
    "We now analyze the distribution of waiting times but considering only the waiting times between an event and the ancestor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE\n",
    "time_diff_tree = np.zeros(N)\n",
    "\n",
    "for d in range(len(v_dict)):\n",
    "    for k in v_dict[d].keys():\n",
    "        # previous vertex has id = k, children vertexes have ids [ v_dict[d][k] ]\n",
    "        for j in v_dict[d][k]:\n",
    "            #print('Computing {}-> {} waiting time.'.format(k,j))\n",
    "            time_diff_tree[int(j)] = df['time'].iloc[int(j)] - df['time'].iloc[int(k)]\n",
    "            \n",
    "time_diff_tree = time_diff_tree[time_diff_tree > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tree, q_tree, p_tree_err, cut_time_tree = plot_powerlaw_hist(time_diff_tree, rescaling = False, \n",
    "                                                                   density = False, cut_off = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the waiting time distribution of consequent events displaces a behaviour similar to the more general case seen above, i.e. a power law with a cut-off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# REMOVE\n",
    "# we choose to study the data for m between 2 and 4.5, because above 5 we get just 104 samples \n",
    "# and there isn't enough information to fit anything but the initial peak of the distribution.\n",
    "ms = np.linspace(2,4,10)\n",
    "p_time_prime = np.zeros(len(ms))\n",
    "q_time_prime = np.zeros(len(ms))\n",
    "p_t_errors_prime = np.zeros(len(ms))\n",
    "cut_times_prime = np.zeros(len(ms))\n",
    "\n",
    "for i in range(len(ms)):\n",
    "\n",
    "    m = ms[i]\n",
    "    print('\\nTime distribution for m = ', m, '\\n')\n",
    "    # waiting time for events of magnitude > m\n",
    "    dfm_pr = df[(df['magnitude'] > m) & (df['prev_event'] == -1)]\n",
    "    timem = np.array(dfm_pr['time'])\n",
    "    timem.sort()\n",
    "    time_d = timem[1:] - timem[:-1]\n",
    "    \n",
    "    # eliminating a couple of anomalous events\n",
    "    temp = time_d[time_d != time_d.max()]\n",
    "    maximum = temp.max()\n",
    "    if time_d.max()*3/4 > maximum:\n",
    "        time_d = temp\n",
    "# COULD BE INTERESTING, BUT IS AN EXTRA PART\n",
    "    p_time_prime[i], q_time_prime[i], p_t_errors_prime[i], cut_times_prime[i] = plot_powerlaw_hist(time_d, rescaling = False, \n",
    "                                                                   density = True, cut_off = True, P0 = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Distance distribution between events\n",
    "\n",
    "Compute the distribution $P_m(r)$ of the distance between an event and the next one, considering earthquakes of magnitude m or above. Also here make a clever choice for the bin sizes and try several values of $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTA\n",
    "Qui potremmo menzionare il fatto che il binning con il metodo di sopra è completamente instabile, che si intravede un picco nella distribuzione, ma non si riescono ad avere un numero sufficiente di bin con una spaziatura adatta per visualizzare il massimo e contemporaneamente fittare separatamente il fronte di salita e discesa. Quindi abbiamo preferito tentare con una decomposizione delle distanze tra quelle di eventi primi e quelle di eventi conseguenti, rispettivamente con una poissoniana e con una powerlaw. In ogni caso le tre celle successive le rimuoverei a prescindere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE\n",
    "def binning_dist(x, rescaling = False, density = False):\n",
    "    \n",
    "    \"\"\"Binning for power laws distributions.\n",
    "        x = entries generated from a power law.\"\"\"\n",
    "    \n",
    "    # x must have streactly positive values; x isn't normalized in general\n",
    "    x = x[x>0]\n",
    "    if rescaling == True:\n",
    "        x = x/x.max()\n",
    "        \n",
    "    x.sort()\n",
    "    # empirical method to get a good amount of bins (min 8), depending on the number of samples\n",
    "    bin_extremes_number = max( min( int( np.log(len(x)) ) , 21), 9) \n",
    "    print('Bin extremes number chosen: ', bin_extremes_number )\n",
    "    first_quantile = x[int(x.shape[0]*(0.3/bin_extremes_number))]\n",
    "    #print('First quantile: ', first_quantile)\n",
    "    bin_extremes = np.logspace(np.log10(first_quantile), np.log10(x.max()), bin_extremes_number)\n",
    "    bin_extremes = np.concatenate((np.array([0]), bin_extremes))\n",
    "\n",
    "    widths = bin_extremes[1:] - bin_extremes[:-1]\n",
    "    centers = (bin_extremes[1:] + bin_extremes[:-1])/2\n",
    "    \n",
    "    freq, _, _ = plt.hist(x, bins=bin_extremes)\n",
    "    plt.close()\n",
    "    \n",
    "    # weights(bin_center) = Prob(bin_center) if normalized\n",
    "    weights = freq/widths\n",
    "    sigma_weights = np.sqrt(freq)/widths\n",
    "    \"\"\"\n",
    "    # merging of the first two bins until we get that the first bin represents the max of the PDF\n",
    "    while weights[0] < weights[1]:\n",
    "        print('Merging first and second bins.')\n",
    "        #this is done by removing the second extreme, thus the first bin becomes the one between 0 and 2\n",
    "        bin_extremes = np.concatenate(([bin_extremes[0]], bin_extremes[2:] ))\n",
    "        widths = bin_extremes[1:] - bin_extremes[:-1]\n",
    "        centers = (bin_extremes[1:] + bin_extremes[:-1])/2\n",
    "\n",
    "        freq, _, _ = plt.hist(x, bins=bin_extremes)\n",
    "        plt.close()\n",
    "        weights = freq/widths\n",
    "        sigma_weights = np.sqrt(freq)/widths\n",
    "    \"\"\"  \n",
    "    # adding also the merging of empty bins with the one on the left\n",
    "    # rewrite it in a better form\n",
    "    mask = (freq != np.zeros(len(freq)))\n",
    "    flag = np.all(mask)\n",
    "    print('All frequencies are non-zero : ', flag)\n",
    "    # should enter in the while loop only in there is at least one bin without counts in it\n",
    "    \n",
    "    while flag == False:\n",
    "        print('Entered in the while loop')\n",
    "        print('Original frequencies: ', freq)\n",
    "        for i in range(1,len(freq)):\n",
    "            if freq[i] == 0:\n",
    "                print('Merging bin {} (empty) with bin {}.'.format(i,i-1))\n",
    "                # bin extremes should be of length len(freq) + 1\n",
    "                # notice that bin_extremes[i] corresponds to the right border of bin[i-1]\n",
    "                # bin_extremes[:i] excludes the bin_extreme[i] !\n",
    "                bin_extremes = np.concatenate((bin_extremes[:i], bin_extremes[i+1:] ))\n",
    "                widths = bin_extremes[1:] - bin_extremes[:-1]\n",
    "                centers = (bin_extremes[1:] + bin_extremes[:-1])/2\n",
    "                # call a break of the for because the len frequence changes and can result in index errors\n",
    "                break\n",
    "                \n",
    "        # update of the frequencies\n",
    "\n",
    "        freq, _, _ = plt.hist(x, bins=bin_extremes)\n",
    "        plt.close()\n",
    "        weights = freq/widths\n",
    "        sigma_weights = np.sqrt(freq)/widths\n",
    "        \n",
    "        mask = (freq != np.zeros(len(freq)))\n",
    "        flag = np.all(mask)\n",
    " \n",
    "    if density == True:\n",
    "        #returns normalized weights (with rescaled errors) so that the area of the histogram is 1\n",
    "        area = np.sum(weights*widths)\n",
    "        weights = weights / area\n",
    "        sigma_weights = sigma_weights/area\n",
    "    \n",
    "    return bin_extremes, widths, centers, freq, weights, sigma_weights   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE\n",
    "def plot_powerlaw_hist_distance(x, rescaling = False, density = False, **kwargs):\n",
    "    \n",
    "    bin_extremes, widths, centers, freq, weights, sigma_weights = binning_dist(x, rescaling, density)\n",
    "    bin_number = len(centers)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,figsize=(11, 5))\n",
    "        \n",
    "    ax1.hist(centers, bins = bin_extremes, weights = weights, histtype = 'step')\n",
    "    ax1.errorbar(centers, weights, sigma_weights, fmt = 'r.')\n",
    "    ax1.set_xscale('log')\n",
    "    #ax1.set_yscale('log')\n",
    "    ax1.set_xlabel('distances [m]', fontsize = 14)\n",
    "    ax1.set_ylabel('occurrencies', fontsize = 14)\n",
    "    ax1.set_title('Number of events = {}'.format(len(x)))\n",
    "    \n",
    "    # now we need to fit the power law in the log-log space, eventually identifying the points before the cut-off\n",
    "    # this should work automatically both for the case rescaling = True or False (if True, x is in [0,1]) \n",
    "    # and for the case density = True or False (if True, the area of the histogram is normalized to 1 \n",
    "    # and the weights are rescaled so that np.sum(weights*bin_widths) = 1)\n",
    "    log_x = np.log(centers)\n",
    "    log_w = np.log(weights)\n",
    "    \n",
    "    # the idea is to write a function that as a default just fits the (log_x,log_w) with a linear function \n",
    "    # log_w = p*log_x + q and has 2 flags: one for excluding skip_initial_pt points (set to 1 for default because\n",
    "    # the first bin is always problematic) and another one to signal that we expect a cut-off at the right side of the\n",
    "    # distribution (i.e. the tail) and we want to stop fitting just before the cut-off.\n",
    "    # we want as a return the parameters p and q with their covariance matrix (that is the default return of \n",
    "    # scipy curve_fit) and, if the cut_off flag is True, also the estimate cut-off (rescaled or not depending on the \n",
    "    # setting passed before)\n",
    "    \n",
    "    if 'cut_off' in kwargs:\n",
    "        if kwargs['cut_off'] == True:\n",
    "             p, q, cov, log_x_cut = loglog_fitting(log_x, log_w, **kwargs)\n",
    "    else:\n",
    "        p, q, cov = loglog_fitting(log_x, log_w, **kwargs)\n",
    "    \n",
    "    # those errors are wrong!\n",
    "    #y_errors = 1./np.sqrt(weights)\n",
    "    #should be\n",
    "    y_errors = sigma_weights/weights\n",
    "    \n",
    "    ax2.errorbar(log_x, log_w, yerr = y_errors ,fmt ='r.', label = 'entries with errors')\n",
    "    ax2.plot(log_x, linear_f(log_x, p, q), \n",
    "             label = 'f(x) = px + q\\np = {} \\nq = {}'.format(round(p,1),round(q,1)))\n",
    "    ax2.legend()\n",
    "    ax2.set_xlabel('distances [logscale]', fontsize = 14)\n",
    "    ax2.set_ylabel('occurrencies [logscale]', fontsize = 14)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if 'cut_off' in kwargs:\n",
    "        if kwargs['cut_off'] == True:\n",
    "            if rescaling == True:\n",
    "                return p, q, np.sqrt(cov[0,0]), np.exp(log_x_cut)*x.max()\n",
    "            else:\n",
    "                return p, q, np.sqrt(cov[0,0]), np.exp(log_x_cut)\n",
    "    else:\n",
    "        # returns the slope, the intercept and the error of the slope\n",
    "        return p, q, np.sqrt(cov[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# REMOVE\n",
    "ms = np.linspace(2,4,6)\n",
    "p_dist = np.zeros(len(ms))\n",
    "q_dist = np.zeros(len(ms))\n",
    "p_d_errors = np.zeros(len(ms))\n",
    "cut_dist = np.zeros(len(ms))\n",
    "\n",
    "\n",
    "for i in range(len(ms)):\n",
    "    m = ms[i]\n",
    "    print('\\nDistance distribution for m = ', m, '\\n')\n",
    "    prime_df = df[df['prev_event'] == -1]\n",
    "    dfm = prime_df[(prime_df['magnitude'] > m) & (prime_df['magnitude'] < m+1)]\n",
    "    X = np.array(dfm[['x','y','z']])\n",
    "    r = np.linalg.norm(X[1:]-X[:-1], axis = 1)\n",
    "    p_dist[i], q_dist[i], p_d_errors[i], cut_dist[i] = plot_powerlaw_hist_distance(r, rescaling = False, \n",
    "                                                        density = False, cut_off = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_bin_number_mod(x, m = 2, min_nbin = 7, fraction = 0.001):\n",
    "    \"\"\"Starts from evenly separed bins and merges the last ones until the tail's counts are \n",
    "        major or equal to the 'fraction' of the total number of occurrencies, given the \n",
    "        constraint that the final number of bins has to be min_nbin.\"\"\"\n",
    "    \n",
    "    # added a factor exp(m-2) to take into account the exponential decrease of total N - empirical formula\n",
    "    n_min = max([int(fraction*len(x)*np.exp(m-2)),10])\n",
    "    print('For m = {} and N = {} the minimum number of events in the tail required is : {}'.format(m, len(x), n_min))\n",
    "    print('Minimum accuracy expected : {}'.format(round(1 - 1/np.sqrt(n_min),2)))\n",
    "    \n",
    "    n, bin_extremes, _ = plt.hist(x, bins = min_nbin )\n",
    "    plt.close()\n",
    "    last_n = n[-1]\n",
    "    \n",
    "    if last_n > n_min:\n",
    "        return min_nbin, bin_extremes\n",
    "    else:\n",
    "        i = min_nbin \n",
    "        nbin = min_nbin\n",
    "        while last_n < n_min and nbin < 100:\n",
    "            nbin = nbin + 1\n",
    "            n, _, _ = plt.hist(x, bins = nbin )\n",
    "            plt.close()\n",
    "            last_n = n[i-1:].sum()\n",
    "        \n",
    "        if last_n > n[min_nbin-2]:\n",
    "            print('-> reducing the final number of bins to {}: \\n'.format(min_nbin - 1))\n",
    "            nbin, bins = select_bin_number_mod(x, m = m, min_nbin = min_nbin - 1)\n",
    "        else:   \n",
    "            n, bin_extremes, _ = plt.hist(x, bins = nbin )\n",
    "            plt.close()\n",
    "            bins = np.concatenate((bin_extremes[:min_nbin],bin_extremes[-1:]))\n",
    "        \n",
    "        return nbin, bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_Pm_r2(m, df):\n",
    "    print('\\nDistance distribution for m = ', m, '\\n')\n",
    "    dfm = df[df['magnitude'] > m]\n",
    "    X = np.array(dfm[['x','y','z']])\n",
    "    r = np.linalg.norm(X[1:]-X[:-1], axis = 1)\n",
    "    r_norm = r/r.max()\n",
    "    # computing suitable sizes of bins\n",
    "    original_bin_number, bins = select_bin_number_mod(r_norm, m=m, min_nbin = 10)\n",
    "    bin_number = len(bins) - 1\n",
    "    \n",
    "    fig, ax1 = plt.subplots(nrows=1, ncols=1,figsize=(6, 5))\n",
    "    \n",
    "    n_tailed, bin_extremes, _ = ax1.hist(r_norm, bins = bins, histtype = 'step')\n",
    "\n",
    "    bin_centers = (bin_extremes[:-1] + bin_extremes[1:])/2\n",
    "    \n",
    "    # rescaling the tail entries with the number of bins merged into the tail \n",
    "    n = np.concatenate((n_tailed[:-1], n_tailed[-1:]/(original_bin_number+1-bin_number)))\n",
    "    \n",
    "    sigma_n = np.sqrt(n)\n",
    "    \n",
    "    ax1.errorbar(bin_centers, n, sigma_n, fmt = 'r.')\n",
    "    ax1.set_xlabel('normalized distances', fontsize = 14)\n",
    "    ax1.set_ylabel('occurrencies', fontsize = 14)\n",
    "    ax1.set_title('Number of events = {}'.format(len(r_norm)))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return r.mean(), r.std()/np.sqrt(len(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ms = np.linspace(2,4.5,6)\n",
    "\n",
    "r_mean = np.zeros(len(ms))\n",
    "r_std = np.zeros(len(ms))\n",
    "\n",
    "for i in range(len(ms)):\n",
    "    m = ms[i]\n",
    "    r_mean[i], r_std[i] = plot_Pm_r2(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance distribution decomposition\n",
    "\n",
    "The distributions that can be seen above can't be easily identified with a known distribution, thus we decomposed it in two different contributions using our knowledge about how the earthquakes relate one to each other. In fact they can be studied as a sequence of independent events (the ones with prev_event = -1), each one of them causing a cluster of related events, as we have already seen in the study of the tree architecture of the dataset.\n",
    "\n",
    "Thus we expect that the overall distance distribution is the result of the following process:\n",
    "* each earthquake has a magnitude distributed as $P(m) \\propto e^{-am}$, $a > 0$;\n",
    "* each earthquake causes N events depending on his magnitude with distibution $N(m) = N_0e^{\\gamma m}$, $\\gamma > 0$;\n",
    "* prime events are independent one from another and are distributed with a certain distance distribution $P'_m(r)$;\n",
    "* consequent events (i.e. all the events that are not prime) have a distance r from the event that caused them, that is distributed with another distribution $P^{cons}(r)$ (here we drop the m-dependence, because a threshold on m is going to breake the chain of cause-effect, that is univoque);\n",
    "\n",
    "This four distributions, if known, could reproduce the probability distribution of the distance between two \"following\" events; thus in this section we are going to study $P'_m(r)$ and $P^{cons}(r).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Distance distribution between prime events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poissonian4(x, A=1, l=1):\n",
    "    from scipy.special import gamma\n",
    "    return A*np.float_power(l,x)/gamma(x)*np.exp(-l)\n",
    "\n",
    "import scipy.stats as st\n",
    "from scipy.integrate import quad\n",
    "\n",
    "class my_pdf(st.rv_continuous):\n",
    "    def _pdf(self,x, A, l):\n",
    "        return poissonian4(x, A, l)  # Normalized over its range, in this case [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_Pm_r_poisson(m, df):\n",
    "    print('\\nDistance distribution for m = ', m, '\\n')\n",
    "    dfm = df[df['magnitude'] > m]\n",
    "    X = np.array(dfm[['x','y','z']])\n",
    "    r = np.linalg.norm(X[1:]-X[:-1], axis = 1)\n",
    "    r_norm = r/r.max()\n",
    "    \n",
    "    \n",
    "    # computing suitable sizes of bins\n",
    "    original_bin_number, bins = select_bin_number_mod(r_norm, m=m, min_nbin = 10)\n",
    "    \n",
    "    bin_number = len(bins) - 1\n",
    "    \n",
    "    fig, ax1 = plt.subplots(nrows=1, ncols=1,figsize=(6, 5))\n",
    "    \n",
    "    n_tailed, bin_extremes, _  = ax1.hist(r_norm, bins = bins, histtype = 'step', density=True)\n",
    "    \n",
    "    bin_centers = (bin_extremes[:-1] + bin_extremes[1:])/2\n",
    "    area = len(r)*(bin_extremes[1] - bin_extremes[0])\n",
    "    # rescaling the tail entries with the number of bins merged into the tail \n",
    "    n = np.concatenate((n_tailed[:-1], n_tailed[-1:]/(original_bin_number+1-bin_number)))\n",
    "    \n",
    "    sigma_n = np.sqrt(n/area)\n",
    "    ax1.errorbar(bin_centers, n, sigma_n, fmt = 'r.', label = 'entries with \\npoissonian errors')\n",
    "    ax1.set_xlabel('normalized distances', fontsize = 14)\n",
    "    ax1.set_ylabel('occurrencies', fontsize = 14)\n",
    "    ax1.set_title('Number of events = {}'.format(len(r_norm)))\n",
    "    \n",
    "    print('Number of bins merged into the tail: {}'.format(original_bin_number - bin_number), '\\n')\n",
    "\n",
    "    #print('bin_centers: ', bin_centers, '\\n')\n",
    "    params1, cov1 = optimize.curve_fit(poissonian4, bin_centers, n, p0 = [n[0], 1])\n",
    "    [A,l] = params1\n",
    "    \n",
    "    # tecnical stuff to create a continuous probability distribution in scipy, that allows for the method .expect\n",
    "    # to compute the expected value of the distribution; the origin of the complication is the fact that poissonians\n",
    "    # usually are discrete and in the continuous case the the parameter lambda doesn't represent anymore the expected value\n",
    "    Area = quad(poissonian4, 0, 1, args=(A, l))[0]\n",
    "    my_cv = my_pdf(a=0, b=1)\n",
    "    \n",
    "    C = A/Area\n",
    "    \n",
    "    x_axis = np.linspace(bin_extremes[0], bin_extremes[-1],100)\n",
    "    ax1.plot(x_axis, my_cv.pdf(x_axis, A=C, l=l), label = 'poissonian \\n $\\lambda$ = %.3f '%l)\n",
    "    x_expected = my_cv.expect(args=(C, l))\n",
    "    x_err = r_norm.std()/np.sqrt(len(r))\n",
    "    ax1.axvline(x_expected, label = 'expected value = %.2f'%x_expected)\n",
    "    \n",
    "    ax1.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # parameters of the poissonian, expected value, error of the mean, max distance for scaling back normalized distances\n",
    "    return C, l, x_expected, x_err, r.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plotted for each magnitude the normalized distances between two events and fitted them with a continuous Poisson distribution, that is usually used for independent event with a well-defined expected value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prime_df = df[df['prev_event'] == -1]\n",
    "# reduced the range of m to [2,4] due to insufficient samples for higher magnitudes\n",
    "pr_ms = np.linspace(2,4,9)\n",
    "\n",
    "pr_Cs = np.zeros(len(pr_ms))\n",
    "pr_ls_r = np.zeros(len(pr_ms))\n",
    "pr_r_expected = np.zeros(len(pr_ms))\n",
    "pr_r_exp_err = np.zeros(len(pr_ms))\n",
    "pr_r_max = np.zeros(len(pr_ms))\n",
    "\n",
    "\n",
    "for i in range(len(pr_ms)):\n",
    "    m = pr_ms[i]\n",
    "    pr_Cs[i], pr_ls_r[i], pr_r_expected[i], pr_r_exp_err[i], pr_r_max[i] = plot_Pm_r_poisson(m, prime_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rescaled_peaks = pr_r_expected*pr_r_max\n",
    "rescaled_errors = pr_r_exp_err*pr_r_max\n",
    "plt.errorbar(pr_ms, rescaled_peaks, rescaled_errors, fmt = 'r.', label = '$x_{max}$')\n",
    "plt.xlabel('magnitude [$T_w$]', fontsize = 14)\n",
    "plt.ylabel('$x_{max}$ of poissonian', fontsize = 14)\n",
    "plt.legend(loc = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph we can see that the expected value of the distance between two events grows linearly as a function of the magnitude until $m = 3.0$, where there is a plateau, indicating some kind of saturation effect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Distance distribution between consequent events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_tree = np.zeros(N)\n",
    "\n",
    "for d in range(len(v_dict)):\n",
    "    for k in v_dict[d].keys():\n",
    "        # previous vertex has id = k, children vertexes have ids [ v_dict[d][k] ]\n",
    "        for j in v_dict[d][k]:\n",
    "            distance_tree[int(j)] = np.linalg.norm(df[['x','y','z']].iloc[int(j)] - df[['x','y','z']].iloc[int(k)])\n",
    "            \n",
    "distance_tree = distance_tree[distance_tree > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the panels below we can see that for consequent events we find once again a powerlaw with a cut-off, just like in the waiting times distribution, even though in this case our analysis can't be performed for all the magnitudes, because this would break the chain of cause-effect that is necessary to the algorithm itself to compute the waiting times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_powerlaw_hist(distance_tree, rescaling = False, density = False, cut_off = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Range-conditioned waiting time distribution\n",
    "\n",
    "Compute the distribution $P_{m,R}(t)$ of waiting times for events of magnitude $m$ or above, which are separated by at most a distance $r<R$, for different values of m and $R$. (In this statistics, if the following event is farther than $R$, skip the $t$ and go to the next pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our analysis we choose to analyze the waiting times distribution for different areas around n fixed centers (uniformly distributed in the plane identified with the PCA procedure).\n",
    "We proceed as follows: \n",
    "* we filter the dataset with the threshold magnitude > m;\n",
    "* for each center $C_i$ and each radius $r_j$ (expressed in fractions of the maximum distance between two events in the dataset) we select only the events whose normalized distance (w.r.t the max distance) from $C_i$ is less than $r_j$; \n",
    "* we compute the waiting times from each event to the one among the remaining that comes right after it (again this is done for n centers and for each radius)\n",
    "* we make a histogram using all the waiting times for a given radius and ALL centers (in this way the resulting distribution is a kind of mean among all centers) and study it as a powerlaw with cutoff (after all is derived from the waiting time distribution seen above, that is a powerlaw with cutoff too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PmR_t(df, m, Rs, n=100, **kwargs):\n",
    "    print('\\nTime distribution for m = ', m, '\\n')\n",
    "    # waiting time for events of magnitude > m\n",
    "    #Xp = np.dot(Vt,X) # last coordinate should be small\n",
    "    #Xpp = np.dot(U, Xp)\n",
    "    centers = np.dot(U, np.array([np.random.uniform(-3,4, n), np.random.uniform(-2,2, n), np.zeros(n)])).T\n",
    "    dfm = df[df['magnitude'] > m]\n",
    "\n",
    "    X = dfm[['x','y','z']].values.T\n",
    "    X = X.astype(\"float64\")\n",
    "    # centering and rescaling the coordinates\n",
    "    for i in range(3):\n",
    "        X[i] = (X[i] - X[i].mean())/X[i].std()\n",
    "\n",
    "    distances = np.linalg.norm((X.T[:,np.newaxis,:] - centers[np.newaxis,:,:]), axis=2)\n",
    "    distances = distances / distances.max()\n",
    "    print(\"Max distance : \", distances.max())\n",
    "    timem = np.array(dfm['time'])\n",
    "    timeM = np.tile(timem[:, np.newaxis], [1,n]).T\n",
    "    \n",
    "    #vector for fit parameters for each R_max fraction\n",
    "    ps = []\n",
    "    qs = []\n",
    "    p_errors = [] \n",
    "    cut_off_times = []\n",
    "    \n",
    "    for i in range(len(Rs)):\n",
    "        #print(timeM.shape)\n",
    "\n",
    "        timeM_filtered = timeM[distances.T < Rs[i]]\n",
    "        #print(timeM.shape)\n",
    "        #print(distances.T.shape)\n",
    "        time_d = (timeM_filtered[1:] - timeM_filtered[:-1])\n",
    "        time_d = time_d[time_d>0]\n",
    "        \n",
    "        p, q, p_err, cut_times = plot_powerlaw_hist(time_d, **kwargs )\n",
    "        \n",
    "        ps.append(p); qs.append(q); p_errors.append(p_err), cut_off_times.append(cut_times)\n",
    "        \n",
    "    return ps, qs, p_errors, cut_off_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook, tnrange\n",
    "import time\n",
    "R_fractions = 20\n",
    "ms = np.arange(2,4.6,0.1)\n",
    "Rs = np.power(np.linspace(np.sqrt(0.1),1,R_fractions),2)\n",
    "Ps = np.zeros((len(ms),R_fractions))\n",
    "Qs = np.zeros((len(ms),R_fractions))\n",
    "P_errors = np.zeros((len(ms),R_fractions))\n",
    "t_cutoff = np.zeros((len(ms),R_fractions))\n",
    "\n",
    "for i in tnrange(len(ms)):\n",
    "    \n",
    "    Ps[i], Qs[i], P_errors[i], t_cutoff[i] = plot_PmR_t(df, ms[i], Rs, cut_off = True, P0 = 5, show = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our analysis of the cutoff times at different radii and magnitudes we can see that there are two different trends: \n",
    "* the cutoff waiting time increases as the magnitude increases (as we saw before with the exponential dependence from m)\n",
    "* the cutoff waiting time increases as the radius decreases\n",
    "\n",
    "The second trend can be interpreted as the fact that background (uncorrelated) events are more rare if we reduce the area of study and this implies a longer correlation between earthquakes in a smaller area (if the magnitude is fixed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize = (10,8))\n",
    "ax = sns.heatmap(np.log10(t_cutoff).T, annot = False, cbar_kws = {'label' : '$log_{10}(t_{cut}[s]) $'})\n",
    "\n",
    "m_index = ['%.1f'%m for m in ms]\n",
    "ax.set_xticklabels(m_index, rotation = 45)\n",
    "ax.set_xlabel('magnitude m [$T_w$]', fontsize = 16)\n",
    "\n",
    "R_index = ['%.2f'%((i+1)/20) for i in range(len(Rs))]\n",
    "ax.set_yticklabels(R_index, rotation = 0)\n",
    "ax.set_ylabel('fraction $R/R_{max}$', fontsize = 16)\n",
    "\n",
    "ax.figure.axes[-1].yaxis.label.set_size(18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Scaling properties\n",
    "Eventually note if, from the analysis of the previous points, there emerges a scaling picture. Is there a suitable rescaling that collapses distributions for various $m$ (and eventually $R$ if point 4 is considered) on a single curve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 - Time scaling with magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapsed_distributions(x, rescaling = False, density = False, show = True, **kwargs):\n",
    "    \n",
    "    bin_extremes, widths, centers, freq, weights, sigma_weights = binning(x, rescaling, density)\n",
    "    bin_number = len(centers)\n",
    "    \n",
    "    if show:\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,figsize=(11, 5))\n",
    "\n",
    "        ax1.hist(centers, bins = bin_extremes, weights = weights, histtype = 'step')\n",
    "        ax1.errorbar(centers, weights, sigma_weights, fmt = 'r.')\n",
    "        ax1.set_xscale('log')\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.set_xlabel('waiting times [s]', fontsize = 14)\n",
    "        ax1.set_ylabel('occurrencies', fontsize = 14)\n",
    "        ax1.set_title('Number of events = {}'.format(len(x)))\n",
    "    \n",
    "    # now we need to fit the power law in the log-log space, eventually identifying the points before the cut-off\n",
    "    # this should work automatically both for the case rescaling = True or False (if True, x is in [0,1]) \n",
    "    # and for the case density = True or False (if True, the area of the histogram is normalized to 1 \n",
    "    # and the weights are rescaled so that np.sum(weights*bin_widths) = 1)\n",
    "    log_x = np.log(centers)\n",
    "    log_w = np.log(weights)\n",
    "    \n",
    "    # the idea is to write a function that as a default just fits the (log_x,log_w) with a linear function \n",
    "    # log_w = p*log_x + q and has 2 flags: one for excluding skip_initial_pt points (set to 1 for default because\n",
    "    # the first bin is always problematic) and another one to signal that we expect a cut-off at the right side of the\n",
    "    # distribution (i.e. the tail) and we want to stop fitting just before the cut-off.\n",
    "    # we want as a return the parameters p and q with their covariance matrix (that is the default return of \n",
    "    # scipy curve_fit) and, if the cut_off flag is True, also the estimate cut-off (rescaled or not depending on the \n",
    "    # setting passed before)\n",
    "    \n",
    "    if 'cut_off' in kwargs:\n",
    "        if kwargs['cut_off'] == True:\n",
    "             p, q, cov, log_x_cut = loglog_fitting(log_x, log_w, **kwargs)\n",
    "    else:\n",
    "        p, q, cov = loglog_fitting(log_x, log_w, **kwargs)\n",
    "    \n",
    "    if show:\n",
    "        y_errors = sigma_weights/weights\n",
    "\n",
    "        ax2.errorbar(log_x, log_w, yerr = y_errors ,fmt ='r.', label = 'entries with errors')\n",
    "        ax2.plot(log_x, linear_f(log_x, p, q), \n",
    "                 label = 'f(x) = px + q\\np = {} \\nq = {}'.format(round(p,1),round(q,1)))\n",
    "        ax2.legend()\n",
    "        ax2.set_xlabel('waiting times [logscale]', fontsize = 14)\n",
    "        ax2.set_ylabel('occurrencies [logscale]', fontsize = 14)\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    if 'cut_off' in kwargs:\n",
    "        if kwargs['cut_off'] == True:\n",
    "            if rescaling == True:\n",
    "                return p, q, np.sqrt(cov[0,0]), np.exp(log_x_cut)*x.max()\n",
    "            else:\n",
    "                return p, q, np.sqrt(cov[0,0]), np.exp(log_x_cut)\n",
    "    else:\n",
    "        # returns the slope, the intercept and the error of the slope\n",
    "        return p, q, np.sqrt(cov[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we anticipated before, if for each magnitude we divide all the time intervals for the correspondent cutoff time (and we rescale the weights multiplying by the cutoff, in order to preserve the normalization of each histogram) we obtain a unique powerlaw with cutoff independent from the magnitude. \n",
    "\n",
    "This means that the system has a scaling property that binds waiting times and magnitudes in a unique way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m_extremes= []\n",
    "m_centers = []\n",
    "m_weights =[]\n",
    "m_sigma= []\n",
    "\n",
    "\n",
    "for i in range(len(ms1)):\n",
    "\n",
    "    m = ms1[i]\n",
    "    dfm = df[df['magnitude'] > m]\n",
    "    timem = np.array(dfm['time'])\n",
    "    timem.sort()\n",
    "    time_d = timem[1:] - timem[:-1]\n",
    "    \n",
    "    # eliminating a couple of anomalous events\n",
    "    temp = time_d[time_d != time_d.max()]\n",
    "    maximum = temp.max()\n",
    "    if time_d.max()*3/4 > maximum:\n",
    "        time_d = temp\n",
    "\n",
    "    bin_extremes, widths, centers, freq, weights, sigma_weights = binning(time_d, rescaling = False, density = True)\n",
    "    m_extremes.append(bin_extremes)\n",
    "    m_centers.append(centers)\n",
    "    m_weights.append(weights)\n",
    "    m_sigma.append(sigma_weights)\n",
    "    \n",
    "extremes_rescaled = []  \n",
    "centers_rescaled = []\n",
    "weights_rescaled =[]\n",
    "sigma_rescaled = []\n",
    "widths_rescaled = []\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "for i in range(len(ms1)):\n",
    "    resc_extremes      = m_extremes[i] / predicted_cut_times[i]; extremes_rescaled.append(resc_extremes)\n",
    "    resc_centers       = m_centers[i] / predicted_cut_times[i];  centers_rescaled.append(resc_centers)\n",
    "    resc_weights       = m_weights[i] * predicted_cut_times[i];  weights_rescaled.append(resc_weights)\n",
    "    resc_sigma_weights = m_sigma[i] * predicted_cut_times[i];    sigma_rescaled.append(resc_sigma_weights)\n",
    "    resc_widths        = resc_extremes[1:] - resc_extremes[:-1]; widths_rescaled.append(resc_widths)\n",
    "    \n",
    "    #centers = m_centers_rescaled[i]/cut_times[i]\n",
    "    #weights = m_weights_rescaled[i]*cut_times[i]\n",
    "    #sigma_weights = m_sigma_rescaled[i]*cut_times[i]\n",
    "    plt.errorbar(resc_centers, resc_weights, yerr = resc_sigma_weights, label = 'm = %.1f'%ms1[i], alpha = 0.9)\n",
    "plt.xlabel('rescaled waiting times '+r'$\\tau$'+' [a.u.]',  fontsize = 16)\n",
    "plt.ylabel('PDF of '+r'$\\tau$', fontsize = 16)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
