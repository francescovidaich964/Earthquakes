{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy as sp\n",
    "from scipy import linalg as la\n",
    "from scipy import optimize\n",
    "from scipy import stats\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'SouthCalifornia-1982-2011_Physics-of-Data.dat'\n",
    "data = np.genfromtxt(file,\n",
    "                     dtype=None,\n",
    "                     delimiter=' ')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.columns = ['event', 'prev_event', 'time', 'magnitude', 'x', 'y', 'z']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Understanding data\n",
    "\n",
    "First of all we took a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that the range of magnitudes is [2,7.3] \n",
    "# and that the measurement of time starts from 0 with the first event\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not all the greatest earthquakes are without ancestors (we shall call them 'prime events')\n",
    "df[df['magnitude']>6].sort_values(by = 'magnitude', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Time distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a routine look at the time distribution and time scale of the dataset \n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(df['event'], df['time'])\n",
    "plt.xlabel('ID number of event')\n",
    "plt.ylabel('time of event [s]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Magnitude distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_expon(x, No, alpha):\n",
    "    return No*np.exp(-alpha*x)\n",
    "\n",
    "def my_log_expon(x, Q, alpha):\n",
    "    return -alpha*x + Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can see how the magnitude is distributed as exp(-m)\n",
    "n, bins, _ = plt.hist(df['magnitude'], log = True, histtype = 'step')\n",
    "bin_centers = (bins[1:] + bins[:-1])/2\n",
    "plt.errorbar(bin_centers, n, np.sqrt(n), fmt = '.b', label = 'values with \\nPoisson error')\n",
    "\n",
    "params1, _ = optimize.curve_fit(my_log_expon, bin_centers, np.log(n))\n",
    "[Q, alpha] = params1\n",
    "No = int(np.exp(Q))\n",
    "plt.plot(bin_centers, my_expon(bin_centers, No, alpha), \n",
    "         label = 'f($M_w$) = $N_o$$e^{-aM_w}$ '+'\\n$N_o$ = {}\\n a = {}'.format(No,round(alpha,2)))\n",
    "\n",
    "plt.xlabel('magnitude [$M_w$]')\n",
    "plt.ylabel('occurrencies [log scale]')\n",
    "plt.ylim(bottom = 1)\n",
    "plt.legend(fontsize = 12)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ADD NON LOGSCALE PLOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Spatial distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#az = [i*15 for i in range(24)]\n",
    "#for azim in az:\n",
    "#%matplotlib nbagg\n",
    "m_bar = 3\n",
    "dfm = df[df['magnitude'] > m_bar]\n",
    "x_coord = dfm['x']\n",
    "y_coord = dfm['y']\n",
    "z_coord = dfm['z']\n",
    "magnitude = dfm['magnitude']\n",
    "\n",
    "fig = plt.figure(figsize = (7,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x_coord/1e06, y_coord/1e06, z_coord/1e06, \n",
    "           s=np.exp(magnitude*1.5)/np.exp(3), c = magnitude, marker=\"o\", alpha = 0.5, \n",
    "           label = 'size of marker \\nproportional \\nto magnitude', cmap = 'plasma')\n",
    "ax.set_xlabel(\"x [$10^6$ m]\", fontsize = 16)\n",
    "ax.set_ylabel(\"y [$10^6$ m]\", fontsize = 16)\n",
    "ax.set_zlabel(\"z [$10^6$ m]\", fontsize = 16)\n",
    "ax.set_title('Spatial distribution of events for m > 3', fontsize = 20)\n",
    "ax.legend(loc = 'center left')\n",
    "#print('azim = ', azim)\n",
    "ax.view_init(elev = 30, azim = 330)\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 PCA for spatial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['x','y','z']].values.T\n",
    "X = X.astype(\"float64\")\n",
    "\n",
    "# centering and rescaling the coordinates\n",
    "for i in range(3):\n",
    "    X[i] = (X[i] - X[i].mean())/X[i].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA \n",
    "C = np.cov(X)\n",
    "U, spectrum, Vt = la.svd(C)\n",
    "print ('Spectrum: ', spectrum,'\\n')\n",
    "perc_expl2 = (spectrum[0] + spectrum[1]) / spectrum.sum()\n",
    "print('Percent of the total variability explained considering the two main features: ', perc_expl2, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projecting the data in the plane generated by the two eigenvectors with the largest eigenvalues\n",
    "Xp = np.dot(Vt,X)\n",
    "\n",
    "%matplotlib inline\n",
    "# general plot of the PCA\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3,\n",
    "                               figsize=(12, 6))\n",
    "\n",
    "[[ax01, ax02, ax03],[ax11, ax12, ax13]] = axes\n",
    "\n",
    "ax01.scatter(X[0], X[1], s= 5)\n",
    "ax01.set_title('x-y projection')\n",
    "ax01.set_xlabel('x')\n",
    "ax01.set_ylabel('y')\n",
    "\n",
    "ax02.scatter(X[0], X[2], s= 5)\n",
    "ax02.set_title('x-z projection')\n",
    "ax02.set_xlabel('x')\n",
    "ax02.set_ylabel('z')\n",
    "\n",
    "ax03.scatter(X[1], X[2], s= 5)\n",
    "ax03.set_title('y-z projection')\n",
    "ax03.set_xlabel('y')\n",
    "ax03.set_ylabel('z')\n",
    "\n",
    "ax11.scatter(Xp[0], Xp[1], s= 5)\n",
    "ax11.set_title('$v_0$-$v_1$ projection')\n",
    "(y_bottom, y_top) = ax11.get_ylim()\n",
    "ax11.set_xlabel('$v_0$')\n",
    "ax11.set_ylabel('$v_1$')\n",
    "\n",
    "ax12.scatter(Xp[0], Xp[2], s= 5)\n",
    "ax12.set_ylim(y_bottom, y_top)\n",
    "ax12.set_title('$v_0$-$v_2$ projection')\n",
    "ax12.set_xlabel('$v_0$')\n",
    "ax12.set_ylabel('$v_2$')\n",
    "\n",
    "ax13.scatter(Xp[1], Xp[2], s= 5)\n",
    "ax13.set_ylim(y_bottom, y_top)\n",
    "ax13.set_title('$v_1$-$v_2$ projection')\n",
    "ax13.set_xlabel('$v_1$')\n",
    "ax13.set_ylabel('$v_2$')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ABBELLIRE\n",
    "# KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[v1, v2, v3] = Vt\n",
    "normal = v3\n",
    "xx, yy = np.meshgrid(range(-4,5), range(-4,5))# calculate corresponding z\n",
    "z = (-normal[0] * xx - normal[1] * yy ) * 1. /normal[2]\n",
    "\n",
    "m_bar = 3\n",
    "dfm = df[df['magnitude'] > m_bar]\n",
    "x_coord = dfm['x']\n",
    "y_coord = dfm['y']\n",
    "z_coord = dfm['z']\n",
    "x_coord = (x_coord - x_coord.mean())/x_coord.std()\n",
    "y_coord = (y_coord - y_coord.mean())/y_coord.std()\n",
    "z_coord = (z_coord - z_coord.mean())/z_coord.std()\n",
    "magnitude = dfm['magnitude']\n",
    "\n",
    "#%matplotlib nbagg\n",
    "\n",
    "fig = plt.figure(figsize = (7,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(xx, yy, z, alpha=0.3)\n",
    "ax.scatter(x_coord, y_coord, z_coord, \n",
    "           s=np.exp(magnitude*1.5)/np.exp(3), c = magnitude, marker=\"o\", alpha = 0.5, \n",
    "           label = 'size of marker \\nproportional \\nto magnitude', cmap = 'plasma')\n",
    "ax.set_xlabel(\"x [$10^6$ m]\", fontsize = 16)\n",
    "ax.set_ylabel(\"y [$10^6$ m]\", fontsize = 16)\n",
    "ax.set_zlabel(\"z [$10^6$ m]\", fontsize = 16)\n",
    "ax.set_title('Spatial distribution of events for m > 3', fontsize = 20)\n",
    "ax.legend(loc = 'center right')\n",
    "#print('azim = ', azim)\n",
    "ax.view_init(elev = 15, azim = 0)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "plt.figure(211)\n",
    "\n",
    "# farlo in 3D con le barre?\n",
    "plt.hist2d(Xp[0], Xp[1], bins = 25, norm = LogNorm(), cmap = \"plasma\")\n",
    "plt.colorbar()\n",
    "plt.xlabel('First principal component', fontsize = 14)\n",
    "plt.ylabel('Second principal component', fontsize = 14)\n",
    "plt.title('Heatmap of events in P.C. coordinates', fontsize = 16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# più fine?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Tree architecture\n",
    "\n",
    "Here we want to exploit the information contained in the 'prev_event' column in order to be able to compute quantities such as distance and waiting time between an event and his ancestor.\n",
    "\n",
    "First of all we iteratively created a nested dictionary, containing as keys the IDs of the primes events (i.e. the ones labelled with 'prev_event' = 1) and as values a dictionary with:\n",
    "* the \"depth\" of the event in the cause-effect chain (i.e. 0 for the prime events, 1 for the ones caused by them, 2 for the ones caused by events of depth 1 and so on so forth);\n",
    "* the \"children\" of that event, i.e. the events that have as prev_event the id of the event considered; \"children\" value contains itself a dictionary like the one just described for the prime events, hence the nested structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evees_light(df, index=-1, depth=0):\n",
    "    dataset = {}\n",
    "    df_filtered = df[df[\"prev_event\"]==index]\n",
    "    lenght =  df_filtered.shape[0]\n",
    "    for i in range(lenght):\n",
    "        event = df_filtered.iloc[i]\n",
    "        dataset[str(int(event[\"event\"]))] = {\"depth\" : depth}\n",
    "        dataset[str(int(event[\"event\"]))][\"children\"] = evees_light(df, index=int(event[\"event\"]), depth = depth + 1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we mapped the dictionary so obtained in a list of dictionaries, whose indexes are the depth of the events (ranging from 0 to max_depth, that is unknown a priori); the entry of the list of index i is a dictionary that has as keys the IDs of the events of depth = i and as values the IDs of its children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dict_array(dataset, dict_array):\n",
    "    for k in dataset.keys():\n",
    "        depth = dataset[k]['depth']\n",
    "        # k is the keyword, children_ids are the values\n",
    "        if len(dict_array) < depth + 1 :\n",
    "            while len(dict_array) < depth + 1:\n",
    "                dict_array.append({})\n",
    "        else:\n",
    "            dict_array[depth][k] = list(dataset[k]['children'].keys())\n",
    "        if len(list(dataset[k]['children'].keys())) != 0:\n",
    "            write_dict_array(dataset[k]['children'], dict_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = evees_light(df)\n",
    "v_dict = [{}]\n",
    "write_dict_array(tree_dataset, v_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the list of dictionaries to add the depth and the number of children (or number of edges if we think of the dataset as an oriented graph) of each event by adding two columns to the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = df.shape[0]\n",
    "edges = np.zeros(N)\n",
    "depths = np.zeros(N)\n",
    "\n",
    "for d in range(len(v_dict)):\n",
    "    for k in v_dict[d].keys():\n",
    "        #print(k)\n",
    "        edges[int(k)] = len(v_dict[d][k])\n",
    "        depths[int(k)] = d\n",
    "        \n",
    "df['edges'] = edges\n",
    "df['depth'] = depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the new features we can see an interesting thing: the event with the highest magnitude (7.3) is not prime \n",
    "# (because it hasn't prev_event = -1), but it happend after a concatenation of 11 earthquakes and subsequently caused \n",
    "# other 4209 events!\n",
    "df[df['magnitude'] == 7.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Furthermore we can see that depth isn't a good indicator of magnitude and that in fact one can't find any relation\n",
    "# between the two\n",
    "mean_mag = df.groupby('depth')['magnitude'].mean()\n",
    "depth_axis = [i for i in range(len(mean_mag))]\n",
    "plt.plot(depth_axis, mean_mag)\n",
    "plt.xlabel('cause-effect depth', fontsize = 14)\n",
    "plt.ylabel('mean magnitude', fontsize = 14)\n",
    "plt.show()\n",
    "\n",
    "# va su e giu perché dopo un po sono pochi eventi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we could expect the events with higher magnitudes have much more edges than the ones with lower magnitudes. \n",
    "# In fact the dependence is exponential with m\n",
    "\n",
    "def my_expon1(x, No, gamma):\n",
    "    return No*np.exp(gamma*x)\n",
    "\n",
    "plt.plot(df['magnitude'], df['edges'] , '.b')\n",
    "plt.xlabel('magnitude [$M_w$]', fontsize = 14)\n",
    "plt.ylabel('number of edges', fontsize = 14)\n",
    "\n",
    "df5 = df[df['magnitude'] > 5]\n",
    "ax = plt.axes([0.3, 0.4, 0.4, 0.4])\n",
    "ax.set_title('Zoom for m > 5')\n",
    "ax.plot(df5['magnitude'], df5['edges'] , '.g')\n",
    "params1, _ = optimize.curve_fit(my_expon1, df5['magnitude'], df5['edges'])\n",
    "[No, gamma] = params1\n",
    "x_axis = np.linspace(5,7.3,100)\n",
    "ax.plot(x_axis, my_expon1(x_axis, *params1), 'r-', \n",
    "        label = '$N_0$exp($\\gamma m$):\\n $\\gamma$ = {}'.format(round(gamma,2)))\n",
    "ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again depth isn't a good indicator of edges (as it wasn't for magnitude)\n",
    "plt.plot(df['depth'], df['edges'], 'b.')\n",
    "plt.xlabel('cause-effect depth', fontsize = 14)\n",
    "plt.ylabel('number of edges', fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Distribution of waiting time \n",
    "Compute the distribution $P_m(t)$ of waiting times for events of magnitude m or above (i.e. do not consider events below $m$). In shaping the bin sizes, take into account that this distribution is expected to have a power-law decay with time (e.g $\\sim 1/t$), and that a power-law is well visualized in log-log scale. Do this analysis for many values of $m$, say $m=2,3,4,5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_bin_number_mod(x, m = 2, min_nbin = 7, fraction = 0.001):\n",
    "    \"\"\"Starts from evenly separed bins and merges the last ones until the tail's counts are \n",
    "        major or equal to the 'fraction' of the total number of occurrencies, given the \n",
    "        constraint that the final number of bins has to be min_nbin.\"\"\"\n",
    "    \n",
    "    # added a factor exp(m-2) to take into account the exponential decrease of total N\n",
    "    n_min = max([int(fraction*len(x)*np.exp(m-2)),10])\n",
    "    print('For m = {} and N = {} the minimum number of events in the tail required is : {}'.format(m, len(x), n_min))\n",
    "    print('Minimum accuracy expected : {}'.format(round(1 - 1/np.sqrt(n_min),2)))\n",
    "    #n_min = fraction*len(x)\n",
    "    \n",
    "    n, bin_extremes, _ = plt.hist(x, bins = min_nbin )\n",
    "    plt.close()\n",
    "    last_n = n[-1]\n",
    "    \n",
    "    if last_n > n_min:\n",
    "        return min_nbin, bin_extremes\n",
    "    else:\n",
    "        i = min_nbin \n",
    "        nbin = min_nbin\n",
    "        while last_n < n_min and nbin < 100:\n",
    "            nbin = nbin + 1\n",
    "            n, _, _ = plt.hist(x, bins = nbin )\n",
    "            plt.close()\n",
    "            last_n = n[i-1:].sum()\n",
    "        \n",
    "        if last_n > n[min_nbin-2]:\n",
    "            print('-> reducing the final number of bins to {}: \\n'.format(min_nbin - 1))\n",
    "            nbin, bins = select_bin_number_mod(x, m = m, min_nbin = min_nbin - 1)\n",
    "        else:   \n",
    "            n, bin_extremes, _ = plt.hist(x, bins = nbin )\n",
    "            plt.close()\n",
    "            bins = np.concatenate((bin_extremes[:min_nbin],bin_extremes[-1:]))\n",
    "        \n",
    "        return nbin, bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_bin_number_mod_mod(x, m, sensibility = 0.1):\n",
    "    x = x/x.max() # normalize data\n",
    "    #nbin = np.floor(len(x)*(sensibility**2))    \n",
    "    #print(nbin)\n",
    "    nbin = 2\n",
    "    if m >= 4.0: sensibility = 0.2\n",
    "        \n",
    "    def select_bins(x, sensibility, nbin):\n",
    "        #adam = 1 / np.exp(np.arange(nbin)).sum()\n",
    "        \n",
    "        if m < 4.0:\n",
    "            # STRATEGY 1 (FOR NOW WORK BETTER UNTIL m = 4.5)\n",
    "            x.sort()\n",
    "            i = int(len(x)/10)\n",
    "            adam = x[i]\n",
    "        else:\n",
    "            # STRATEGY 2 (WORK WELL FOR BIG m)\n",
    "            x.sort()\n",
    "            i = int(2/(sensibility**2))\n",
    "            adam = x[i]\n",
    "        \n",
    "        #print(\"adam: \", adam)\n",
    "        \n",
    "        bin_extremes = np.exp(np.linspace(np.log(adam)+1, 1, nbin))/np.e # \n",
    "        bin_extremes = np.concatenate((np.array([0]),bin_extremes))\n",
    "        #print(\"bin_extremes: \", bin_extremes)\n",
    "        widths = bin_extremes[1:] - bin_extremes[:-1]\n",
    "        centers = (bin_extremes[1:] + bin_extremes[:-1])/2\n",
    "        freq, _, _ = plt.hist(x, bins=bin_extremes)\n",
    "        weights = freq/widths\n",
    "        plt.close()\n",
    "        #print(\"frequencies: \", freq)\n",
    "        sigma = 1/np.sqrt(freq)\n",
    "        #print(freq)\n",
    "        return bin_extremes, widths, centers, freq, weights, sigma\n",
    "    \n",
    "    while(True):\n",
    "        _, _, _, _, _, sigma = select_bins(x, sensibility, nbin)\n",
    "        #print(\"sigma: \", sigma)\n",
    "        if sigma.max() > sensibility:\n",
    "            #print(\"nbin: \", nbin)\n",
    "            bin_extremes, widths, centers, freq, weights, sigma = select_bins(x, sensibility, nbin - 1 )\n",
    "            break\n",
    "        nbin += 1\n",
    "    \n",
    "    print(nbin)\n",
    "    #plt.hist(centers, bins=bin_extremes, weights=freq/h)\n",
    "    #plt.show()\n",
    "    return bin_extremes, widths, centers, freq, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look if they are all used or if there is some useless\n",
    "def logP(t, A, B):\n",
    "    return A + B*t\n",
    "\n",
    "def logPmod(t, A, B, C):\n",
    "    return A + B/t**C\n",
    "\n",
    "def logt(t, p, q, r):\n",
    "    return p*np.log(t*r) + q\n",
    "\n",
    "def loglogt(t, p, q):\n",
    "    return p*t + q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_Pm_t2(m, df, sensibility = 0.1):\n",
    "    print('\\nTime distribution for m = ', m, '\\n')\n",
    "    # waiting time for events of magnitude > m\n",
    "    dfm = df[df['magnitude'] > m]\n",
    "    timem = np.array(dfm['time'])\n",
    "    timem.sort()\n",
    "    time_d = timem[1:] - timem[:-1]\n",
    "    \n",
    "    # eliminating a couple of anomalous events\n",
    "    temp = time_d[time_d != time_d.max()]\n",
    "    maximum = temp.max()\n",
    "    if time_d.max()*3/4 > maximum:\n",
    "        time_d = temp\n",
    "    \n",
    "    # time differences rescaled to [0,1]\n",
    "    time_diff = time_d/time_d.max()\n",
    "    \n",
    "    # computing suitable sizes and number of bins\n",
    "    bin_extremes, widths, centers, freq, weights = select_bin_number_mod_mod(time_diff, m, sensibility)\n",
    "    bin_number = len(centers)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,figsize=(11, 5))\n",
    "    \n",
    "    ax1.hist(centers, bins = bin_extremes, weights = weights, histtype = 'step')\n",
    "    #bin_centers = (bin_extremes[:-1] + bin_extremes[1:])/2\n",
    "    \n",
    "    # rescaling the tail entries with the number of bins merged into the tail \n",
    "    # -> it's not fair, the rescaling should depend on the distribution expected\n",
    "    #n = np.concatenate((n_tailed[:-1], n_tailed[-1:]/(original_bin_number+1-bin_number)))\n",
    "    sigma_weights = np.sqrt(freq)/widths\n",
    "    ax1.errorbar(centers, weights, sigma_weights, fmt = 'r.')\n",
    "    #, label = 'entries with \\npoisson error'    #ax1.legend(loc = 'upper right')\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_xlabel('normalized waiting times', fontsize = 14)\n",
    "    ax1.set_ylabel('occurrencies', fontsize = 14)\n",
    "    ax1.set_title('Number of events = {}'.format(len(time_diff)))\n",
    "    \n",
    "    # adding an inner plot of the relative accuracy (i.e. 1 - rel.err.) for the entries of the histogram\n",
    "    \"\"\"ax3 = plt.axes([0.32, 0.55, 0.15, 0.25])\n",
    "    accuracies = 1 - 1./np.sqrt(n)\n",
    "    plt.plot(bin_centers, accuracies, 'g.')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('bin labels')\n",
    "    plt.setp(ax3, xticks = bin_centers, xticklabels =  [i+1 for i in range(len(bin_centers))])\n",
    "    \"\"\"\n",
    "    #sq_error = np.zeros(len(centers)-2)\n",
    "    print(len(centers))\n",
    "    abs_err = np.zeros(len(centers)-3)\n",
    "    for points in range(3,len(centers)):\n",
    "        # linear fit over the log of the entries (trying every cut-off)\n",
    "        params1, cov1 = optimize.curve_fit(loglogt, np.log(centers)[:points], np.log(weights)[:points], p0 = [-1,5])\n",
    "        [p,q] = params1\n",
    "        var_p = np.diag(cov1)[0]\n",
    "        \n",
    "        #sq_error[cut_i-1] = mean_squared_error(np.log(weights)[:-cut_i], loglogt(np.log(centers)[:-cut_i], p,q)) * (len(centers)-cut_i)\n",
    "        abs_err[points-3] = np.abs( np.log(weights)[:points] - loglogt(np.log(centers)[:points], p,q) ).sum()\n",
    "        \n",
    "        \n",
    "    perc_error = (abs_err[1:]-abs_err[:-1])/abs_err[:-1]\n",
    "    print(perc_error)\n",
    "    \"\"\"\n",
    "    for i in range(len(perc_error)-1):\n",
    "        if perc_error[i]<perc_error[i+1]:\n",
    "            good_points = (i+3)+1\n",
    "            break\n",
    "    \"\"\"\n",
    "    treshold = 0.15\n",
    "    good_points = 3\n",
    "    for i in range(1,len(perc_error)):\n",
    "        #print(perc_error[-i])        \n",
    "        if perc_error[-i]<treshold:\n",
    "            good_points = len(perc_error)-(i-1) +3\n",
    "            print(good_points)\n",
    "            break\n",
    "        \n",
    "    \n",
    "    print('Optimal cut given removing',len(centers)-(good_points+1),'points')\n",
    "    params1, cov1 = optimize.curve_fit(loglogt, np.log(centers)[:good_points], np.log(weights)[:good_points], p0 = [-1,5])\n",
    "    [p,q] = params1\n",
    "    var_p = np.diag(cov1)[0]\n",
    "            \n",
    "    y_errors = 1./np.sqrt(weights)\n",
    "    ax2.errorbar(np.log(centers), np.log(weights), yerr = y_errors ,fmt ='r.', label = 'entries with errors')\n",
    "    ax2.plot(np.log(centers), loglogt(np.log(centers), *params1), \n",
    "             label = 'f(x) = px + q\\np = {} \\nq = {}'.format(round(params1[0],1),round(params1[1],1)))\n",
    "             #label = 'p = {} \\nq = {}\\n$\\chi^2$ = {} \\np-value = {}'\\\n",
    "             #.format(round(params1[0],1),round(params1[1],1),round(chisq,2),round(pv,2)))\n",
    "    ax2.legend()\n",
    "    ax2.set_xlabel('normalized waiting times [logscale]', fontsize = 14)\n",
    "    ax2.set_ylabel('occurrencies [logscale]', fontsize = 14)\n",
    "    \n",
    "    # Finally we can plot in the first panel the obtained fit:\n",
    "    #x_axis = np.linspace(bin_centers[0], bin_extremes[-1], 100)\n",
    "    #print(ax1)\n",
    "    #ax1.plot(x_axis, np.exp(p*np.log(x_axis)+q))\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(time_d.max())\n",
    "    #print(bin_extremes[-cut_i])\n",
    "    return p, q, np.sqrt(var_p)#, bin_extremes[-cut_i]*time_d.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we choose to study the data for m between 2 and 4.5, because above 5 we get just 104 samples \n",
    "# and there isn't enough information to fit anything but the initial peak of the distribution.\n",
    "\n",
    "ms = np.linspace(2,4.5,12)\n",
    "ps1 = np.zeros(len(ms))\n",
    "qs1 = np.zeros(len(ms))\n",
    "p_errors = np.zeros(len(ms))\n",
    "cut_times = np.zeros(len(ms))\n",
    "# we could also use the covariance of the parameters from curve_fit in order to have an error on p !\n",
    "\n",
    "#cut_i = 5\n",
    "for i in range(len(ms)):\n",
    "    #if cut_i != 1:\n",
    "    #    cut_i -= 1\n",
    "        \n",
    "    m = ms[i]\n",
    "    ps1[i], qs1[i], p_errors[i] = plot_Pm_t2(m, df, sensibility = 0.1)\n",
    "\n",
    "    #ps1[i], qs1[i], p_errors[i], cut_times[i] = plot_Pm_t2(m, df, cut_i, sensibility = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Scaling of the exponent as a function of m\n",
    "\n",
    "Distribution considered:\n",
    "\n",
    "$P_{m>\\bar{m}}(t_w) = At_w^{-\\alpha(\\bar{m})}$,\n",
    "\n",
    "where $\\alpha = -p$ and A is just a normalization constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see how the exponent is a linear funcion of m\n",
    "print(ms, ps1)\n",
    "#ms = ms[:-2]\n",
    "#ps1 = ps1[:-2]\n",
    "#p_errors = p_errors[:-2]\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(ms, ps1)\n",
    "plt.errorbar(ms, ps1, yerr = p_errors, fmt = '.r', label = 'estimated exponents \\nwith errors' )\n",
    "plt.plot(ms, intercept+slope*ms, label = 'fit: p(m) = %.2fm%.2f'%(slope,intercept))\n",
    "#plt.title('Dipendence on magnitude of the exponent p')\n",
    "plt.ylabel('exponent p', fontsize = 14)\n",
    "plt.xlabel('magnitude m', fontsize = 14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(ms, np.log(cut_times))\n",
    "plt.plot(ms, np.exp(intercept+slope*ms), label = 'fit: p(m) = %.2fm + %.2f'%(slope,intercept))\n",
    "\n",
    "plt.errorbar(ms, cut_times, fmt = '.r')\n",
    "#plt.title('Dipendence on magnitude of the exponent p')\n",
    "plt.ylabel('Waiting time cut-off', fontsize = 14)\n",
    "plt.xlabel('magnitude m', fontsize = 14)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "print(cut_times)\n",
    "plt.show()\n",
    "\n",
    "#fiko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Waiting time distribution between concatenated events\n",
    "\n",
    "We now analyze the distribution of waiting times but considering only the waiting times between an event and the ancestor.\n",
    "\n",
    "\n",
    "In general we expect a greater exponent $\\alpha$ of $P_{m>\\bar{m}}(t_w) = At_w^{-\\alpha(\\bar{m})}$ because we consider much more correlated pairs of event than in the previous case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diff_tree = np.zeros(N)\n",
    "\n",
    "for d in range(len(v_dict)):\n",
    "    for k in v_dict[d].keys():\n",
    "        # previous vertex has id = k, children vertexes have ids [ v_dict[d][k] ]\n",
    "        for j in v_dict[d][k]:\n",
    "            #print('Computing {}-> {} waiting time.'.format(k,j))\n",
    "            time_diff_tree[int(j)] = df['time'].iloc[int(j)] - df['time'].iloc[int(k)]\n",
    "            \n",
    "time_diff_tree = time_diff_tree[time_diff_tree > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_Pm_t_tree(time_d, cut_i=1):\n",
    "    # time differences rescaled to [0,1]\n",
    "    time_diff = time_d/time_d.max() \n",
    "    \n",
    "    # computing suitable sizes of bins\n",
    "    m = 2\n",
    "    bin_extremes, widths, centers, freq, weights = select_bin_number_mod_mod(time_diff, m = m, sensibility = 0.1)\n",
    "    bin_number = len(centers)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,figsize=(11, 5))\n",
    "    \n",
    "    ax1.hist(centers, bins = bin_extremes, weights=weights, histtype = 'step')\n",
    "    \n",
    "    #n = np.concatenate((n_tailed[:-1], n_tailed[-1:]/(original_bin_number+1-bin_number)))\n",
    "    sigma_weights = np.sqrt(freq)/widths\n",
    "    ax1.errorbar(centers, weights, sigma_weights, fmt = 'r.')\n",
    "    #, label = 'entries with \\npoisson error'    #ax1.legend(loc = 'upper right')\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_xlabel('normalized waiting times', fontsize = 14)\n",
    "    ax1.set_ylabel('occurrencies', fontsize = 14)\n",
    "    ax1.set_title('Number of events = {}'.format(len(time_diff)))\n",
    "    \n",
    "    # adding an inner plot of the relative accuracy (i.e. 1 - rel.err.) for the entries of the histogram\n",
    "    \"\"\"ax3 = plt.axes([0.32, 0.55, 0.15, 0.25])\n",
    "    accuracies = 1 - 1./np.sqrt(n)\n",
    "    plt.plot(bin_centers, accuracies, 'g.')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('bin labels')\n",
    "    #plt.ylabel('accuracy')\n",
    "    #plt.ylim(bottom = 0, top = 1)\n",
    "    plt.setp(ax3, xticks = bin_centers, xticklabels =  [i+1 for i in range(len(bin_centers))])\n",
    "    \"\"\"\n",
    "    #print('Occurrencies for the first bin with {} original bins: '.format(bin_number), int(n[0]), '\\n')\n",
    "    #print('Number of bins merged into the tail: {}'.format(original_bin_number - bin_number), '\\n')\n",
    "\n",
    "    # rescaling the tail entries with the number of bins merged into the tail \n",
    "    # -> it's not fair, the rescaling should depend on the distribution expected\n",
    "    #n = np.concatenate((n[:-1], n[-1:]/(original_bin_number+1-bin_number)))\n",
    "    # linear fit over the log of the entries\n",
    "    params1, _ = optimize.curve_fit(loglogt, np.log(centers)[:-cut_i], np.log(weights)[:-cut_i], p0 = [-1,20])\n",
    "    [p,q] = params1\n",
    "    #print('p = {}\\nq = {}'.format(round(p,2), round(q,2)))\n",
    "    \n",
    "    # chi squared and p-value tests\n",
    "    #chisq, pv = chisquare(np.log(n), loglogt(np.log(bin_centers), *params1), ddof = 2 )\n",
    "    \n",
    "    y_errors = 1./np.sqrt(weights)\n",
    "    ax2.errorbar(np.log(centers), np.log(weights), yerr = y_errors ,fmt ='r.', label = 'entries with errors')\n",
    "    #print(np.log(centers))\n",
    "    #print(loglogt(np.log(centers), *params1))\n",
    "    ax2.plot(np.log(centers), loglogt(np.log(centers), *params1), \n",
    "             label = 'f(x) = px + q\\np = {} \\nq = {}'.format(round(params1[0],1),round(params1[1],1)))\n",
    "             #label = 'p = {} \\nq = {}\\n$\\chi^2$ = {} \\np-value = {}'\\\n",
    "             #.format(round(params1[0],1),round(params1[1],1),round(chisq,2),round(pv,2)))\n",
    "    ax2.legend()\n",
    "    ax2.set_xlabel('normalized waiting times [logscale]', fontsize = 14)\n",
    "    ax2.set_ylabel('occurrencies [logscale]', fontsize = 14)\n",
    "    \n",
    "    # Finally we can plot in the first panel the obtained fit:\n",
    "    x_axis = np.linspace(centers[0], centers[-1], 100)\n",
    "    #print(ax1)\n",
    "    #ax1.plot(x_axis, np.exp(p*np.log(x_axis)+q))\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return p, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tree, q_tree = plot_Pm_t_tree(time_diff_tree, cut_i=5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Distance distribution between events\n",
    "\n",
    "Compute the distribution $P_m(r)$ of the distance between an event and the next one, considering earthquakes of magnitude m or above. Also here make a clever choice for the bin sizes and try several values of $m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_Pm_r2(m, df):\n",
    "    print('\\nDistance distribution for m = ', m, '\\n')\n",
    "    dfm = df[df['magnitude'] > m]\n",
    "    X = np.array(dfm[['x','y','z']])\n",
    "    r = np.linalg.norm(X[1:]-X[:-1], axis = 1)\n",
    "    r_norm = r/r.max()\n",
    "    \n",
    "    # computing suitable sizes of bins\n",
    "    bin_extremes, widths, centers, freq, weights = select_bin_number_mod_mod(r_norm, m = m, sensibility = 0.03)\n",
    "    #original_bin_number, bins = select_bin_number_mod(r_norm, m=m, min_nbin = 10)\n",
    "    bin_number = len(centers)\n",
    "    \n",
    "    fig, ax1 = plt.subplots(nrows=1, ncols=1,figsize=(6, 5))\n",
    "    \n",
    "    ax1.hist(centers, bins = bin_extremes, weights = weights, histtype = 'step')\n",
    "    #bin_centers = (bin_extremes[:-1] + bin_extremes[1:])/2\n",
    "    \n",
    "    # rescaling the tail entries with the number of bins merged into the tail \n",
    "    # -> it's not fair, the rescaling should depend on the distribution expected\n",
    "    #n = np.concatenate((n_tailed[:-1], n_tailed[-1:]/(original_bin_number+1-bin_number)))\n",
    "    sigma_weights = np.sqrt(freq)/widths\n",
    "    ax1.errorbar(centers, weights, sigma_weights, fmt = 'r.')\n",
    "    ax1.set_xlabel('normalized distances', fontsize = 14)\n",
    "    ax1.set_ylabel('occurrencies', fontsize = 14)\n",
    "    ax1.set_title('Number of events = {}'.format(len(r_norm)))\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    #print('Number of bins merged into the tail: {}'.format(original_bin_number - bin_number), '\\n')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return r.mean(), r.std()/np.sqrt(len(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ms = np.linspace(2,4.5,6)\n",
    "\n",
    "r_mean = np.zeros(len(ms))\n",
    "r_std = np.zeros(len(ms))\n",
    "\n",
    "for i in range(len(ms)):\n",
    "    m = ms[i]\n",
    "    r_mean[i], r_std[i] = plot_Pm_r2(m, df)\n",
    "    #plot_Pm_r2(m, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance distribution decomposition\n",
    "\n",
    "The distributions that can be seen above can't easily identified with a known distribution, thus we decomposed it in two different contributions using our knowledge about how the earthquakes relate one to each other, i.e. they can be studied as a sequence of independent events (the ones with prev_event = -1) each one of them causing a cluster of related events, as we have already seen in the study of the tree architecture of the dataset.\n",
    "\n",
    "Thus we expect that the overall distance distribution is the result of the following process:\n",
    "* each earthquake has a magnitude distributed as $P(m) \\propto e^{-am}$, $a > 0$;\n",
    "* each earthquake causes N events depending on his magnitude with distibution $N(m) = N_0e^{\\gamma m}$, $\\gamma > 0$;\n",
    "* prime events are independent one from another and are distributed with a certain distance distribution $P'_m(r)$;\n",
    "* consequent events (i.e. all the events that are not prime) have a distance r from the event that caused them, that is distributed with another distribution $P^{cons}(r)$ (here we drop the m-dependence because a threshold on m is going to breake the chain of cause-effect that is univoque);\n",
    "\n",
    "This four distributions, if known, could reproduce the probability distribution of the distance between two \"following\" events; thus in this section we are going to study $P'_m(r)$ and $P^{cons}(r).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Distance distribution between prime events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poissonian4(x, A=1, l=1):\n",
    "    from scipy.special import gamma\n",
    "    return A*np.float_power(l,x)/gamma(x)*np.exp(-l)\n",
    "\n",
    "import scipy.stats as st\n",
    "from scipy.integrate import quad\n",
    "\n",
    "class my_pdf(st.rv_continuous):\n",
    "    def _pdf(self,x, A, l):\n",
    "        return poissonian4(x, A, l)  # Normalized over its range, in this case [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(st.rv_continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_Pm_r_poisson(m, df):\n",
    "    print('\\nDistance distribution for m = ', m, '\\n')\n",
    "    dfm = df[df['magnitude'] > m]\n",
    "    X = np.array(dfm[['x','y','z']])\n",
    "    r = np.linalg.norm(X[1:]-X[:-1], axis = 1)\n",
    "    r_norm = r/r.max()\n",
    "    \n",
    "    \n",
    "    # computing suitable sizes of bins\n",
    "    original_bin_number, bins = select_bin_number_mod(r_norm, m=m, min_nbin = 10)\n",
    "    \n",
    "    bin_number = len(bins) - 1\n",
    "    \n",
    "    fig, ax1 = plt.subplots(nrows=1, ncols=1,figsize=(6, 5))\n",
    "    \n",
    "    n_tailed, bin_extremes, _  = ax1.hist(r_norm, bins = bins, histtype = 'step', density=True)\n",
    "    \n",
    "    bin_centers = (bin_extremes[:-1] + bin_extremes[1:])/2\n",
    "    area = len(r)*(bin_extremes[1] - bin_extremes[0])\n",
    "    # rescaling the tail entries with the number of bins merged into the tail \n",
    "    # -> it's not fair, the rescaling should depend on the distribution expected\n",
    "    n = np.concatenate((n_tailed[:-1], n_tailed[-1:]/(original_bin_number+1-bin_number)))\n",
    "    \n",
    "    sigma_n = np.sqrt(n/area)\n",
    "    ax1.errorbar(bin_centers, n, sigma_n, fmt = 'r.', label = 'entries with \\npoissonian errors')\n",
    "    ax1.set_xlabel('normalized distances', fontsize = 14)\n",
    "    ax1.set_ylabel('occurrencies', fontsize = 14)\n",
    "    ax1.set_title('Number of events = {}'.format(len(r_norm)))\n",
    "    \n",
    "    print('Number of bins merged into the tail: {}'.format(original_bin_number - bin_number), '\\n')\n",
    "\n",
    "    print('bin_centers: ', bin_centers, '\\n')\n",
    "    params1, cov1 = optimize.curve_fit(poissonian4, bin_centers, n, p0 = [n[0], 1])\n",
    "    [A,l] = params1\n",
    "\n",
    "    Area = quad(poissonian4, 0, 1, args=(A, l))[0]\n",
    "    my_cv = my_pdf(a=0, b=1)\n",
    "    \n",
    "    C = A/Area\n",
    "    \n",
    "    x_axis = np.linspace(bin_extremes[0], bin_extremes[-1],100)\n",
    "    ax1.plot(x_axis, my_cv.pdf(x_axis, A=C, l=l), label = 'poissonian \\n $\\lambda$ = %.2f '%l)\n",
    "    x_expected = my_cv.expect(args=(C, l))\n",
    "    x_err = r_norm.std()/np.sqrt(len(r))\n",
    "    ax1.axvline(x_expected, label = 'expected value = %.3f'%x_expected)\n",
    "    \n",
    "    ax1.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    #expected value, parameters of the poissonian\n",
    "    return C, l, x_expected, x_err, r.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prime_df = df[df['prev_event'] == -1]\n",
    "# reduced the range of m to [2,4] due to insufficient samples for higher magnitudes\n",
    "pr_ms = np.linspace(2,4,9)\n",
    "\n",
    "pr_Cs = np.zeros(len(pr_ms))\n",
    "pr_ls_r = np.zeros(len(pr_ms))\n",
    "pr_r_expected = np.zeros(len(pr_ms))\n",
    "pr_r_exp_err = np.zeros(len(pr_ms))\n",
    "pr_r_max = np.zeros(len(pr_ms))\n",
    "\n",
    "\n",
    "for i in range(len(pr_ms)):\n",
    "    m = pr_ms[i]\n",
    "    pr_Cs[i], pr_ls_r[i], pr_r_expected[i], pr_r_exp_err[i], pr_r_max[i] = plot_Pm_r_poisson(m, prime_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rescaled_peaks = pr_r_expected*pr_r_max\n",
    "rescaled_errors = pr_r_exp_err*pr_r_max\n",
    "plt.errorbar(pr_ms, rescaled_peaks, rescaled_errors, fmt = 'r.', label = '$x_{max}$')\n",
    "plt.xlabel('magnitude [$T_w$]', fontsize = 14)\n",
    "plt.ylabel('$x_{max}$ of poissonian', fontsize = 14)\n",
    "plt.legend(loc = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Distance distribution between consequent events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_tree = np.zeros(N)\n",
    "\n",
    "for d in range(len(v_dict)):\n",
    "    for k in v_dict[d].keys():\n",
    "        # previous vertex has id = k, children vertexes have ids [ v_dict[d][k] ]\n",
    "        for j in v_dict[d][k]:\n",
    "            distance_tree[int(j)] = np.linalg.norm(df[['x','y','z']].iloc[int(j)] - df[['x','y','z']].iloc[int(k)])\n",
    "            \n",
    "distance_tree = distance_tree[distance_tree > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want now to analyse the core of the distribution, neglecting the tail for the moment\n",
    "print('Max distance: ', distance_tree.max())\n",
    "dist_truncated = distance_tree[distance_tree < 100000]\n",
    "samples_discarded = len(distance_tree)-len(dist_truncated)\n",
    "percentage_discarded = samples_discarded / len(distance_tree)\n",
    "print('Number of samples discarded : {} ({}%)'.format(samples_discarded, round(percentage_discarded*100,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, _ = plt.hist(dist_truncated, bins = 20, histtype = 'step')\n",
    "plt.xlabel('distance [km]', fontsize = 14)\n",
    "plt.ylabel('occurrencies', fontsize = 14)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution is characterized by a decrease that is more than exponential in the first part and then has a long tail that is non-neglegible. Furthermore using the logscale in the y axis we can see that the exponent of the distribution is a convex function of the distance. This considerations lead us to model the distribution as a Cauchy distribution \n",
    "$P(r) = \\frac{1}{\\pi D_r}(1 + \\frac{r^2}{D_r^2})^{-1}$\n",
    "with a typical displacement $D_r$ that in principle should depend on the characteristic waiting time between two events but we take it just as a parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_cauchy_distribution(x, a, D_x):\n",
    "    return a - np.log(1 + np.power(x/D_x,2))\n",
    "\n",
    "def cauchy_distribution(x, A, D_x):\n",
    "    return (A/D_x)/(1 + np.power(x/D_x,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_number = 20\n",
    "n, bins, _ = plt.hist(dist_truncated, bins = bin_number)\n",
    "plt.close()\n",
    "bin_centers = (bins[:-1] + bins[1:])/2\n",
    "params1, cov1 = optimize.curve_fit(log_cauchy_distribution, bin_centers, np.log(n), p0 = [np.log(n[0]), 100])\n",
    "[a,D_x] = params1\n",
    "D_x_err = np.sqrt(np.diag(cov1)[1])\n",
    "\n",
    "x_axis = np.linspace(bin_centers[0], bin_centers[-1],100)\n",
    "plt.hist(dist_truncated, bins = bin_number, histtype = 'step')\n",
    "plt.plot(x_axis, np.exp(log_cauchy_distribution(x_axis,*params1)), \n",
    "         label = 'Cauchy distribution \\n$D_x = %.0f$'%(D_x))\n",
    "plt.errorbar(bin_centers, n, yerr = np.sqrt(n), fmt = 'r.', label = 'occurrencies')\n",
    "plt.xlabel('distances [km]', fontsize = 14)\n",
    "plt.ylabel('occurrencies', fontsize = 14)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Range-conditioned waiting time distribution\n",
    "\n",
    "Compute the distribution $P_{m,R}(t)$ of waiting times for events of magnitude $m$ or above, which are separated by at most a distance $r<R$, for different values of m and $R$. (In this statistics, if the following event is farther than $R$, skip the $t$ and go to the next pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot waiting times with R condition (fixed m_min)\n",
    "\n",
    "def Pm_t_Rcondition(df, m, R_fractions):\n",
    "    #print('m = ', m, '\\n')\n",
    "    \n",
    "    # distance and waiting times between events of magnitude > m\n",
    "    dfm = df[df['magnitude'] > m]\n",
    "    dfm = dfm.set_index(np.arange(len(dfm)))\n",
    "    X = np.array(dfm[['x','y','z']])\n",
    "    r = np.linalg.norm(X[1:]-X[:-1], axis = 1)\n",
    "    timem = np.array(dfm['time'])\n",
    "    timem.sort()\n",
    "    time_d = timem[1:] - timem[:-1]\n",
    "    \n",
    "    #vector for fit parameters for each R_max fraction\n",
    "    ps = []\n",
    "    qs = []\n",
    "    p_errors = [] \n",
    "    Rs = np.power(np.linspace(np.sqrt(0.1),1,R_fractions),2)\n",
    "    print('R_max fractions considered: ', Rs, '\\n')\n",
    "    #Repeat plots and fits for each R_max fraction\n",
    "    # fra code\n",
    "    #for i in range(1, R_fractions+1):\n",
    "    for i in range(R_fractions):\n",
    "        # building the mask to filter \"distances < R\" and apply that to original waiting times\n",
    "        print(\"R_max = \"+str(i+1)+\"/\"+str(R_fractions)+\" * max_distance\")\n",
    "        # linearly spaced fractions - fra code\n",
    "        #R_mask = pd.Series(np.concatenate((np.array([True]), r <= i/R_fractions*r.max())))\n",
    "        R_mask = pd.Series(np.concatenate((np.array([True]), r <= Rs[i]*r.max())))\n",
    "        dfmr = dfm[R_mask]\n",
    "        r_norm = r[R_mask[1:]]/r.max()  \n",
    "        time_diff = time_d[R_mask[1:]]/time_d.max()\n",
    "        print(\"{} events hav been removed ({}%)\\n\"\\\n",
    "              .format(len(dfm)-len(dfmr), round(100*(1-(len(dfmr)/len(dfm))),2)))\n",
    " \n",
    "        # computing suitable sizes of bins\n",
    "        original_bin_number, bins = select_bin_number_mod(time_diff, m = m)\n",
    "        bin_number = len(bins) - 1\n",
    "\n",
    "        n_tailed, bin_extremes, _  = plt.hist(time_diff, bins = bins, histtype = 'step')\n",
    "\n",
    "        bin_centers = (bin_extremes[:-1] + bin_extremes[1:])/2\n",
    "\n",
    "         # rescaling the tail entries with the number of bins merged into the tail \n",
    "        # -> it's not fair, the rescaling should depend on the distribution expected\n",
    "        n = np.concatenate((n_tailed[:-1], n_tailed[-1:]/(original_bin_number+1-bin_number)))\n",
    "\n",
    "        # linear fit over the log of the entries\n",
    "        params1, cov1 = optimize.curve_fit(loglogt, np.log(bin_centers), np.log(n), p0 = [-1,5])\n",
    "        [p,q] = params1\n",
    "        var_p = np.diag(cov1)[0]\n",
    "        print('variance of exponent p = ', var_p)\n",
    "        plt.close()\n",
    "\n",
    "        #Add parameters in their lists\n",
    "        ps.append(p)\n",
    "        qs.append(q)\n",
    "        p_errors.append(np.sqrt(var_p))\n",
    "    return np.array(ps), np.array(qs), np.array(p_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ms = ms = np.linspace(2,4.5,6)\n",
    "R_fractions = 20\n",
    "Ps = np.zeros((len(ms),R_fractions))\n",
    "Qs = np.zeros((len(ms),R_fractions))\n",
    "P_errors = np.zeros((len(ms),R_fractions))\n",
    "\n",
    "for i in range(len(ms)):\n",
    "    Ps[i], Qs[i], P_errors[i] = Pm_t_Rcondition(df, ms[i], R_fractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "n = 100\n",
    "m = 3\n",
    "\n",
    "#Xp = np.dot(Vt,X) # last coordinate should be small\n",
    "#Xpp = np.dot(U, Xp)\n",
    "centers = np.dot(U, np.array([np.random.uniform(-3,4, n), np.random.uniform(-2,2, n), np.zeros(n)])).T\n",
    "dfm = df[df['magnitude'] > m]\n",
    "\n",
    "X = dfm[['x','y','z']].values.T\n",
    "X = X.astype(\"float64\")\n",
    "# centering and rescaling the coordinates\n",
    "for i in range(3):\n",
    "    X[i] = (X[i] - X[i].mean())/X[i].std()\n",
    "\n",
    "distances = np.linalg.norm((X.T[:,np.newaxis,:] - centers[np.newaxis,:,:]), axis=2)\n",
    "R = int(distances.mean()*5)\n",
    "timem = np.array(dfm['time'])\n",
    "timeM = np.tile(timem[:, np.newaxis], [1,100]).T\n",
    "timeM = timeM[distances.T < R]\n",
    "time_diff = (timeM[1:] - timeM[:-1])\n",
    "time_diff = time_diff[time_diff>0]\n",
    "\n",
    "plt.hist(time_diff)\n",
    "plt.yscale(\"log\")\n",
    "print(centers[0:5])\n",
    "print(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tnrange, tqdm_notebook\n",
    "from time import sleep\n",
    "for i in tnrange(4, desc='1st loop'):\n",
    "    for j in tnrange(100, desc='2nd loop'):\n",
    "        sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the beginning of the notebook\n",
    "xx, yy = np.meshgrid(range(-4,5), range(-4,5))\n",
    "def plane(x, y):\n",
    "    return (-v3[0] * x - v3[1] * y) * 1. /v3[2]\n",
    "#m_bar = 3\n",
    "#dfm = df[df['magnitude'] > m_bar]\n",
    "#x_coord = dfm['x']\n",
    "#y_coord = dfm['y']\n",
    "#z_coord = dfm['z']\n",
    "#x_coord = (x_coord - x_coord.mean())/x_coord.std()\n",
    "#y_coord = (y_coord - y_coord.mean())/y_coord.std()\n",
    "#z_coord = (z_coord - z_coord.mean())/z_coord.std()\n",
    "#magnitude = dfm['magnitude']\n",
    "\n",
    "#%matplotlib nbagg\n",
    "\n",
    "fig = plt.figure(figsize = (7,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(xx, yy, plane(xx, yy), alpha=0.3)\n",
    "(x, y) = (np.random.random(100), np.random.random(100))\n",
    "ax.scatter(centers[:,0], centers[:,1], plane(centers[:,0], centers[:,1]))\n",
    "#print('azim = ', azim)\n",
    "ax.view_init(elev = 15, azim = 0)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Show the joint distribution using kernel density estimation\n",
    "g = sns.jointplot(Xp[0], Xp[1], kind=\"kde\", xlim=[-3,4], ylim=[-2,2], height=7, space=0)\n",
    "g.set_axis_labels(xlabel='x', ylabel='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our analysis there isn't a neither a clear nor a signficant dependence of the parameter p of $P_{m,R}(t) = t^p$ (p < 0) from the chosen R.\n",
    "\n",
    "In order to show that we plot in the left panel the value of p as a function of $\\frac{R}{R_{max}}$ (for R that goes from 0.1 $R_{max}$ to $R_{max}$) for each m considered and in the right panel the mean value of p(R,m) (averaged over R) as a function of m and confront it with the prediction for p(m) obtained from $P_m(t)$ in section 2.\n",
    "\n",
    "We can see that there isn't any clear pattern in the left panel, and that fluctuations tend to decrease as $\\frac{R}{R_{max}}$ approaches to 1; Furthermore almost all the average points $p_{mean}(m)$ are compatible within 2 sigmas with the prediction considering all ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rs = np.power(np.linspace(np.sqrt(0.1),1,R_fractions),2)\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,figsize=(11, 5))\n",
    "for i in range(1,len(ms)+1):\n",
    "    #plt.errorbar(Rs, Ps[-i], yerr = P_errors[-i], label = 'm = {}'.format(ms[-i]))\n",
    "    ax1.plot(Rs, Ps[-i], label = 'm = {}'.format(ms[-i]))\n",
    "ax1.legend(loc = 'lower right')\n",
    "ax1.set_title('p exponent of $P_{m,R}(t)$')\n",
    "ax1.set_xlabel('$R_{max}$ fraction', fontsize = 14)\n",
    "ax1.set_ylabel('p exponent', fontsize = 14)\n",
    "ax1.set_ylim(-3.3,-1.7)*\n",
    "#log scale can be useful because the P(r) distribution isn't uniform\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "ax2.errorbar(ms, Ps.mean(axis = 1), yerr = Ps.std(axis = 1)/np.sqrt(len(ms)), fmt = 'r.', label = '$p_{mean}(m)$')\n",
    "ax2.plot(ms, intercept+slope*ms, label = 'fit: p(m) = %.2fm%.2f'%(slope,intercept))\n",
    "ax2.set_title('$<p(m,R)>_R$ mean exponent vs $P_{m}(t)$ prediction')\n",
    "ax2.set_ylabel('exponent $<p>_R$', fontsize = 14)\n",
    "ax2.set_xlabel('magnitude m', fontsize = 14)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Scaling properties\n",
    "Eventually note if, from the analysis of the previous points, there emerges a scaling picture. Is there a suitable rescaling that collapses distributions for various $m$ (and eventually $R$ if point 4 is considered) on a single curve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 - Time scaling with magnitude\n",
    "First of all we have seen that the exponent $p$ of $P_m(t)$ is linear in m: $p(m) = \\alpha + m\\beta$\n",
    "\n",
    "This mean that we can write:  \n",
    "\n",
    "$P_m(t) = At^{p(m)} = At^{\\alpha + m\\beta} = P_0(t)t^{m\\beta}$\n",
    "\n",
    "Thus, through the scaling $t_w \\rightarrow t_w^{-m\\beta}t_w$ we should get the same distribution for the waiting time of events of magnitude > m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
